{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d6d8016-f6ec-4b80-b261-c73fc774a099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb62a6c-a8a4-481d-a77f-75319729e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from typing import List, Dict, Any, TypedDict\n",
    "from langchain_community.document_loaders import TextLoader, PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Weaviate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668b7cb4-2711-48c5-b4b5-b24caeb3ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_INGEST = False\n",
    "k = 20\n",
    "n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e47ec807-37e4-4063-8695-c15c99f21c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = CharacterTextSplitter(chunk_size=5000, chunk_overlap=100)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa34e53-b008-41ba-813c-2e6c08442ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1757593739330.pdf\n",
      "Processing: 2505.06416v1 SCALEMCP - DYNAMIC AND AUTO SYNCHRONIZING MODEL CONTEXT PROTOCOL TOOLS FOR LLM AGENTS.pdf\n",
      "Processing: A Tutorial on LLM. Generative artificial intelligence‚Ä¶ _ by Haifeng Li _ Medium.pdf\n",
      "Processing: ACID Properties In DBMS Explained _ MongoDB _ MongoDB.pdf\n",
      "Processing: Agentic_Design_Patterns.pdf\n",
      "Processing: All you need to know about ‚ÄòAttention‚Äô and ‚ÄòTransformers‚Äô ‚Äî In-depth Understanding ‚Äî Part 1 _ by Arjun Sarkar _ Towards Data Science.pdf\n",
      "Processing: All you need to know about ‚ÄòAttention‚Äô and ‚ÄòTransformers‚Äô ‚Äî In-depth Understanding ‚Äî Part 2 _ by Arjun Sarkar _ Towards Data Science.pdf\n",
      "Processing: Applying word2vec to Recommenders and Advertising ¬∑ Chris McCormick.pdf\n",
      "Processing: Build your own Transformer from scratch using Pytorch _ by Arjun Sarkar _ Towards Data Science.pdf\n",
      "Processing: Foundations of LLMs.pdf\n",
      "Processing: linear algebra - How to intuitively understand eigenvalue and eigenvector_ - Mathematics Stack Exchange.pdf\n",
      "Processing: MCP_new.pdf\n",
      "Processing: Methods to Estimate Large Language Model Confidence.pdf\n",
      "Processing: ML_Ai_Cheatsheets.pdf\n",
      "Processing: pca - Making sense of principal component analysis, eigenvectors & eigenvalues - Cross Validated.pdf\n",
      "Processing: Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf\n",
      "Processing: Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf\n",
      "Processing: Step by Step of Principal Component Analysis _ by Everton Gomede, PhD _ Medium.pdf\n",
      "Processing: Transformers from Scratch.pdf\n",
      "Processing: Unsupervised Feature Learning and Deep Learning Tutorial.pdf\n",
      "Processing: What is regularization_ _ IBM.pdf\n",
      "Processing: Word2Vec Tutorial - The Skip-Gram Model ¬∑ Chris McCormick.pdf\n",
      "Processing: Word2Vec Tutorial Part 2 - Negative Sampling ¬∑ Chris McCormick.pdf\n",
      "Processing: üèé Smaller, faster, cheaper, lighter_ Introducing DistilBERT, a distilled version of BERT _ by Victor Sanh _ HuggingFace _ Medium.pdf\n"
     ]
    }
   ],
   "source": [
    "import pymupdf\n",
    "\n",
    "folder_path = \"kb_ip\"\n",
    "\n",
    "text_content_dict = dict()\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".pdf\"):  # Check if the file is a PDF\n",
    "        pdf_path = os.path.join(folder_path, file_name)\n",
    "        text_pdf = str()\n",
    "        print(f\"Processing: {file_name}\")\n",
    "        \n",
    "        # Open the PDF file\n",
    "        with pymupdf.open(pdf_path) as pdf_document:\n",
    "            for page_num in range(len(pdf_document)):  # Loop through all pages\n",
    "                page = pdf_document[page_num]\n",
    "                text = page.get_text()  # Extract text from the page\n",
    "                text_pdf += text\n",
    "                # print(f\"Page {page_num + 1}:\\n{text}\\n\")\n",
    "\n",
    "        # # Original file name\n",
    "        # file_name = \"example.txt\"\n",
    "        \n",
    "        # # Split the file name into root and extension\n",
    "        # file_root, file_ext = os.path.splitext(pdf_path)\n",
    "        \n",
    "        # # Replace the extension\n",
    "        # new_file_name = f\"{file_root}.txt\"\n",
    "        \n",
    "        # with open(new_file_name, 'w', encoding='utf-8') as file:\n",
    "        #     file.write(text_pdf)#.encode('utf-8'))\n",
    "        \n",
    "        loader = PyMuPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        text_content_dict[file_name] = chunks\n",
    "        \n",
    "        del loader, documents, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849e218e-1c59-4cd0-8608-1c448a4596a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# pprint(text_content_dict['1757593739330.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "084490a3-8e0a-49c0-8c95-0e570809482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [value for sublist in text_content_dict.values() for value in sublist]\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "748b68b3-b83b-468e-9677-da73e85518b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9bf5c04-c767-4513-b0a6-0f97d9bc274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "faf4280c-6acc-4a50-a51f-c0a4d4431cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_relevance_score_fn(cosine_dist):\n",
    "    # return cosine_dist\n",
    "    return 0\n",
    "    # return 1-cosine_dist\n",
    "    # return (cosine_dist + 1.0) / 2.0\n",
    "    # return 1-((cosine_dist + 1.0) / 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba13e3c6-cef7-4223-b309-06fdc0ee0ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChromaDB vector store\n",
    "vectorstore = Chroma(collection_name=\"my_rag_cosine\", \n",
    "                     embedding_function=embeddings, \n",
    "                     persist_directory=\"chromadb_cosine\",\n",
    "                     relevance_score_fn=custom_relevance_score_fn,\n",
    "                     collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "234d24cb-269f-4f02-97b3-aefc772cea1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No ingestion\n"
     ]
    }
   ],
   "source": [
    "if DO_INGEST:\n",
    "    # vectorstore.add_texts([doc.page_content for doc in documents], metadatas=[doc.metadata for doc in documents])\n",
    "    # vectorstore.add_documents(documents=documents)\n",
    "    \n",
    "    # Add documents in a loop\n",
    "    print(f\"Total Documents: {len(documents)}\")\n",
    "    for i in range(len(documents)):\n",
    "        doc = documents[i]\n",
    "        print(f\"Ingesting Document: {i}\")\n",
    "        vectorstore.add_documents([doc])    \n",
    "    print(\"Documents ingested to the vector store!\")\n",
    "\n",
    "else:\n",
    "    print(\"No ingestion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7a6de5-e386-4cc0-a84a-256605ea1c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f78897-d3ed-4624-ba37-3f034c93f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Similarity Search:\n",
    "\n",
    "# results = vectorstore.similarity_search(\n",
    "#     query=\"What is regularization?\", k=k\n",
    "# )\n",
    "\n",
    "# for res in results:\n",
    "#     print(f\"Content: {res.page_content} \\n\\nMetadata: {res.metadata}\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584a4d70-28a4-49aa-8669-88e6b8bc99b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f542f76-1ec1-4864-92da-3a29153190f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Search by Vector:\n",
    "\n",
    "# query_vector = embeddings.embed_query(\"What is regularization?\")\n",
    "# results = vectorstore.similarity_search_by_vector(embedding=query_vector, k=k)\n",
    "\n",
    "# for res in results:\n",
    "#     print(f\"Content: {res.page_content} \\n\\nMetadata: {res.metadata}\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc90fff-0341-49cb-aaeb-b7a46e7b9edc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11620962-f140-4884-9be8-b32350aff431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Document(id='e33b18f1-b8db-45bd-b74b-4fe812ff7733', metadata={'page': 0, 'keywords': '', 'title': '', 'author': '', 'total_pages': 8, 'producer': 'Skia/PDF m120', 'creationDate': \"D:20240127055328+00'00'\", 'format': 'PDF 1.4', 'creationdate': '2024-01-27T05:53:28+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'modDate': \"D:20240127055328+00'00'\", 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'moddate': '2024-01-27T05:53:28+00:00', 'subject': '', 'trapped': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf'}, page_content='Explore watsonx.ai\\nWhat is regularization?\\nOverview'), 0)\n",
      "Content: Explore watsonx.ai\n",
      "What is regularization?\n",
      "Overview \n",
      "\n",
      "Metadata: {'page': 0, 'keywords': '', 'title': '', 'author': '', 'total_pages': 8, 'producer': 'Skia/PDF m120', 'creationDate': \"D:20240127055328+00'00'\", 'format': 'PDF 1.4', 'creationdate': '2024-01-27T05:53:28+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'modDate': \"D:20240127055328+00'00'\", 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'moddate': '2024-01-27T05:53:28+00:00', 'subject': '', 'trapped': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='5b21daa2-90aa-49b5-9dfa-f8cc58968131', metadata={'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'trapped': '', 'modDate': \"D:20240127055328+00'00'\", 'creationDate': \"D:20240127055328+00'00'\", 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'author': '', 'producer': 'Skia/PDF m120', 'page': 1, 'moddate': '2024-01-27T05:53:28+00:00', 'total_pages': 8, 'format': 'PDF 1.4', 'subject': '', 'creationdate': '2024-01-27T05:53:28+00:00', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'title': '', 'keywords': ''}, page_content='Published:\\xa016 November 2023\\nContributors: Jacob Murel Ph.D., Eda Kavlakoglu\\nRegularization is a set of methods for reducing overfitting in machine\\nlearning models. Typically, regularization trades a marginal decrease in\\ntraining accuracy for an increase in generalizability.\\nRegularization encompasses a range of techniques to correct for overfitting in machine\\n As such, regularization is a method for increasing a model‚Äôs\\nlearning models.\\ngeneralizability‚Äîthat is, it‚Äôs ability to produce accurate predictions on new datasets.1\\nRegularization provides this increased generalizability at the sake of increased training\\nerror. In other words, regularization methods typically lead to less accurate predictions\\non training data but more accurate predictions on test data.\\nRegularization differs from optimization. Essentially, the former increases model\\ngeneralizability while the latter increases model training accuracy. Both are important\\nconcepts in machine learning and data science.\\nThere are many forms of regularization. Anything in the way of a complete guide requires\\na much longer book-length treatment. Nevertheless, this article provides an overview of\\nthe theory necessary to understand regularization‚Äôs purpose in machine learning as well\\nas a survey of several popular regularization techniques.\\nBias-variance tradeoff\\nTypes of regularization with linear models\\nTypes of regularization in machine learning\\nProducts\\nResources\\nTake the next step'), 0)\n",
      "Content: Published:¬†16 November 2023\n",
      "Contributors: Jacob Murel Ph.D., Eda Kavlakoglu\n",
      "Regularization is a set of methods for reducing overfitting in machine\n",
      "learning models. Typically, regularization trades a marginal decrease in\n",
      "training accuracy for an increase in generalizability.\n",
      "Regularization encompasses a range of techniques to correct for overfitting in machine\n",
      " As such, regularization is a method for increasing a model‚Äôs\n",
      "learning models.\n",
      "generalizability‚Äîthat is, it‚Äôs ability to produce accurate predictions on new datasets.1\n",
      "Regularization provides this increased generalizability at the sake of increased training\n",
      "error. In other words, regularization methods typically lead to less accurate predictions\n",
      "on training data but more accurate predictions on test data.\n",
      "Regularization differs from optimization. Essentially, the former increases model\n",
      "generalizability while the latter increases model training accuracy. Both are important\n",
      "concepts in machine learning and data science.\n",
      "There are many forms of regularization. Anything in the way of a complete guide requires\n",
      "a much longer book-length treatment. Nevertheless, this article provides an overview of\n",
      "the theory necessary to understand regularization‚Äôs purpose in machine learning as well\n",
      "as a survey of several popular regularization techniques.\n",
      "Bias-variance tradeoff\n",
      "Types of regularization with linear models\n",
      "Types of regularization in machine learning\n",
      "Products\n",
      "Resources\n",
      "Take the next step \n",
      "\n",
      "Metadata: {'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'trapped': '', 'modDate': \"D:20240127055328+00'00'\", 'creationDate': \"D:20240127055328+00'00'\", 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'author': '', 'producer': 'Skia/PDF m120', 'page': 1, 'moddate': '2024-01-27T05:53:28+00:00', 'total_pages': 8, 'format': 'PDF 1.4', 'subject': '', 'creationdate': '2024-01-27T05:53:28+00:00', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'title': '', 'keywords': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='6ff9919a-ffc7-429a-90eb-bb83d409fa94', metadata={'title': '', 'format': 'PDF 1.4', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'trapped': '', 'creationDate': \"D:20240127060753+00'00'\", 'keywords': '', 'subject': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'producer': 'Skia/PDF m120', 'moddate': '2024-01-27T06:07:53+00:00', 'creationdate': '2024-01-27T06:07:53+00:00', 'modDate': \"D:20240127060753+00'00'\", 'page': 0, 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'total_pages': 5, 'author': ''}, page_content='Regularization. What, Why, When,\\nand How?\\nIn this article, I want to take an in-depth look at regularization.\\nAkash Shastri ¬∑ Follow\\nPublished in Towards Data Science ¬∑ 5 min read ¬∑ Oct 24, 2020\\n132\\nWhat?\\nWhat is regularization?\\nRegularization is a method to constraint the model to fit our data accurately\\nand not overfit. It can also be thought of as penalizing unnecessary\\ncomplexity in our model. There are mainly 3 types of regularization\\ntechniques deep learning practitioners use. They are:\\n1. L1 Regularization or Lasso regularization\\n2. L2 Regularization or Ridge regularization\\n3. Dropout\\nSidebar: Other techniques can also have a regularizing effect on our model. You\\ncan prevent overfitting by also having more data to constraint the search space of\\nour function. This can be done with techniques like data augmentation, that\\ncreate more data to train, hence reducing overfitting.\\nThere are many other solutions to overfitting such as ensembling and stopping\\nearly, that can help prevent overfitting but are not considered regularization as'), 0)\n",
      "Content: Regularization. What, Why, When,\n",
      "and How?\n",
      "In this article, I want to take an in-depth look at regularization.\n",
      "Akash Shastri ¬∑ Follow\n",
      "Published in Towards Data Science ¬∑ 5 min read ¬∑ Oct 24, 2020\n",
      "132\n",
      "What?\n",
      "What is regularization?\n",
      "Regularization is a method to constraint the model to fit our data accurately\n",
      "and not overfit. It can also be thought of as penalizing unnecessary\n",
      "complexity in our model. There are mainly 3 types of regularization\n",
      "techniques deep learning practitioners use. They are:\n",
      "1. L1 Regularization or Lasso regularization\n",
      "2. L2 Regularization or Ridge regularization\n",
      "3. Dropout\n",
      "Sidebar: Other techniques can also have a regularizing effect on our model. You\n",
      "can prevent overfitting by also having more data to constraint the search space of\n",
      "our function. This can be done with techniques like data augmentation, that\n",
      "create more data to train, hence reducing overfitting.\n",
      "There are many other solutions to overfitting such as ensembling and stopping\n",
      "early, that can help prevent overfitting but are not considered regularization as \n",
      "\n",
      "Metadata: {'title': '', 'format': 'PDF 1.4', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'trapped': '', 'creationDate': \"D:20240127060753+00'00'\", 'keywords': '', 'subject': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'producer': 'Skia/PDF m120', 'moddate': '2024-01-27T06:07:53+00:00', 'creationdate': '2024-01-27T06:07:53+00:00', 'modDate': \"D:20240127060753+00'00'\", 'page': 0, 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'total_pages': 5, 'author': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='475a19a6-2993-4eeb-9fa1-fb455e4eaf31', metadata={'modDate': \"D:20240127060753+00'00'\", 'creationdate': '2024-01-27T06:07:53+00:00', 'page': 2, 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'subject': '', 'title': '', 'trapped': '', 'total_pages': 5, 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'producer': 'Skia/PDF m120', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'keywords': '', 'moddate': '2024-01-27T06:07:53+00:00', 'creationDate': \"D:20240127060753+00'00'\", 'author': ''}, page_content='When?\\nWhen do we use regularization?\\nWe use regularization whenever we suspect our model is overfitting. The\\nbiggest signs of overfitting are the poor performance of validation metrics.\\nThe validation set is part of our dataset that the model has not yet seen.\\nAs we want to detect if our model is learning just from the data, or is being\\nheavily influenced by noise, we use the validation set which has different\\nnoise than our training set. So if our model were to overfit the training data,\\nit would predict poorly on our validation set.\\nDuring training, we also constantly measure validation metrics. If we see the\\nvalidation metrics not improving significantly, or worsening, this is a telltale\\nsign that our model is overfitting. We need to then apply regularization\\ntechniques.\\nNote: Some regularization techniques have no downside, and should be used ALL\\nthe time. An example of this is data augmentation. There‚Äôs no downside to using\\ndata augmentation and should be used regardless of whether model is overfitting.\\nHow?\\nL1 Regularization\\nL1 regularization works by adding a penalty based on the absolute value of\\nparameters scaled by some value l (typically referred to as lambda).\\nInitially our loss function was: Loss = f(preds,y)\\nWhere y is the target output, and preds is the prediction\\npreds = WX + b, where W is parameters, X is input and b is bias.\\nWith L1 regularization we add an extra term of l*|W|, where W is the weight\\nmatrix (parameters). So our loss function after L1 regularization is'), 0)\n",
      "Content: When?\n",
      "When do we use regularization?\n",
      "We use regularization whenever we suspect our model is overfitting. The\n",
      "biggest signs of overfitting are the poor performance of validation metrics.\n",
      "The validation set is part of our dataset that the model has not yet seen.\n",
      "As we want to detect if our model is learning just from the data, or is being\n",
      "heavily influenced by noise, we use the validation set which has different\n",
      "noise than our training set. So if our model were to overfit the training data,\n",
      "it would predict poorly on our validation set.\n",
      "During training, we also constantly measure validation metrics. If we see the\n",
      "validation metrics not improving significantly, or worsening, this is a telltale\n",
      "sign that our model is overfitting. We need to then apply regularization\n",
      "techniques.\n",
      "Note: Some regularization techniques have no downside, and should be used ALL\n",
      "the time. An example of this is data augmentation. There‚Äôs no downside to using\n",
      "data augmentation and should be used regardless of whether model is overfitting.\n",
      "How?\n",
      "L1 Regularization\n",
      "L1 regularization works by adding a penalty based on the absolute value of\n",
      "parameters scaled by some value l (typically referred to as lambda).\n",
      "Initially our loss function was: Loss = f(preds,y)\n",
      "Where y is the target output, and preds is the prediction\n",
      "preds = WX + b, where W is parameters, X is input and b is bias.\n",
      "With L1 regularization we add an extra term of l*|W|, where W is the weight\n",
      "matrix (parameters). So our loss function after L1 regularization is \n",
      "\n",
      "Metadata: {'modDate': \"D:20240127060753+00'00'\", 'creationdate': '2024-01-27T06:07:53+00:00', 'page': 2, 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'subject': '', 'title': '', 'trapped': '', 'total_pages': 5, 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'producer': 'Skia/PDF m120', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'keywords': '', 'moddate': '2024-01-27T06:07:53+00:00', 'creationDate': \"D:20240127060753+00'00'\", 'author': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='a6f72249-96b0-41bd-b066-7fee33095662', metadata={'title': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'producer': 'Skia/PDF m120', 'subject': '', 'total_pages': 5, 'trapped': '', 'creationDate': \"D:20240127060753+00'00'\", 'moddate': '2024-01-27T06:07:53+00:00', 'modDate': \"D:20240127060753+00'00'\", 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'author': '', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'creationdate': '2024-01-27T06:07:53+00:00', 'page': 1, 'keywords': ''}, page_content='they do not constraint the search space or penalize complexity. Although you\\nshould keep these in mind as regularization is not the only way to cure overfitting.\\nWhy?\\nWhy do we need regularization?\\nFig 1: Underfit v overfit. Image by the author. Inspired by Andrew Ng\\nIn Fig 1, we see 3 curves. The one on the left is doing a poor job of predicting\\nthe points, and the one on the right is doing a ‚ÄúTOO GOOD‚Äù job of predicting\\nthe points. We can intuitively tell the left graph isn‚Äôt right, but why is the\\nright one bad? Isn‚Äôt it good that our model predicts points exactly where they\\nare?\\nThe answer is NO, and here‚Äôs why. Our data contains some noise, we do not\\nwant our model to predict noise, as noise is random. So the graph on the\\nright is not ideal either, we want something like the graph in the middle.\\nUnderfitting is caused due to our model being too simple, or not being\\ntrained long enough. Overfitting is a harder problem.\\nOverfitting can be caused either due to an overly complex model that‚Äôs\\nlearning noise, or the search space for our model function is large and we do\\nnot have enough data to constraint the search.\\nSo regularization is a way to stop overfitting.\\nOpen in app\\nSign up\\nSign in\\nSearch\\nWrite'), 0)\n",
      "Content: they do not constraint the search space or penalize complexity. Although you\n",
      "should keep these in mind as regularization is not the only way to cure overfitting.\n",
      "Why?\n",
      "Why do we need regularization?\n",
      "Fig 1: Underfit v overfit. Image by the author. Inspired by Andrew Ng\n",
      "In Fig 1, we see 3 curves. The one on the left is doing a poor job of predicting\n",
      "the points, and the one on the right is doing a ‚ÄúTOO GOOD‚Äù job of predicting\n",
      "the points. We can intuitively tell the left graph isn‚Äôt right, but why is the\n",
      "right one bad? Isn‚Äôt it good that our model predicts points exactly where they\n",
      "are?\n",
      "The answer is NO, and here‚Äôs why. Our data contains some noise, we do not\n",
      "want our model to predict noise, as noise is random. So the graph on the\n",
      "right is not ideal either, we want something like the graph in the middle.\n",
      "Underfitting is caused due to our model being too simple, or not being\n",
      "trained long enough. Overfitting is a harder problem.\n",
      "Overfitting can be caused either due to an overly complex model that‚Äôs\n",
      "learning noise, or the search space for our model function is large and we do\n",
      "not have enough data to constraint the search.\n",
      "So regularization is a way to stop overfitting.\n",
      "Open in app\n",
      "Sign up\n",
      "Sign in\n",
      "Search\n",
      "Write \n",
      "\n",
      "Metadata: {'title': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'producer': 'Skia/PDF m120', 'subject': '', 'total_pages': 5, 'trapped': '', 'creationDate': \"D:20240127060753+00'00'\", 'moddate': '2024-01-27T06:07:53+00:00', 'modDate': \"D:20240127060753+00'00'\", 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'author': '', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'creationdate': '2024-01-27T06:07:53+00:00', 'page': 1, 'keywords': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='73fd666d-c75d-46d3-bd95-b96210005ccf', metadata={'creationDate': \"D:20240120084858+00'00'\", 'keywords': '', 'modDate': \"D:20240120084858+00'00'\", 'creationdate': '2024-01-20T08:48:58+00:00', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'title': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'trapped': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'total_pages': 6, 'page': 0, 'format': 'PDF 1.4', 'moddate': '2024-01-20T08:48:58+00:00', 'subject': '', 'producer': 'Skia/PDF m120', 'author': ''}, page_content='Regularization in Machine Learning\\nPrashant Gupta ¬∑ Follow\\nPublished in Towards Data Science ¬∑ 7 min read ¬∑ Nov 15, 2017\\n9.8K\\n43\\nOne of the major aspects of training your machine learning model is\\navoiding overfitting. The model will have a low accuracy if it is overfitting. This\\nhappens because your model is trying too hard to capture the noise in your\\ntraining dataset. By noise we mean the data points that don‚Äôt really represent\\nthe true properties of your data, but random chance. Learning such data\\npoints, makes your model more flexible, at the risk of overfitting.'), 0)\n",
      "Content: Regularization in Machine Learning\n",
      "Prashant Gupta ¬∑ Follow\n",
      "Published in Towards Data Science ¬∑ 7 min read ¬∑ Nov 15, 2017\n",
      "9.8K\n",
      "43\n",
      "One of the major aspects of training your machine learning model is\n",
      "avoiding overfitting. The model will have a low accuracy if it is overfitting. This\n",
      "happens because your model is trying too hard to capture the noise in your\n",
      "training dataset. By noise we mean the data points that don‚Äôt really represent\n",
      "the true properties of your data, but random chance. Learning such data\n",
      "points, makes your model more flexible, at the risk of overfitting. \n",
      "\n",
      "Metadata: {'creationDate': \"D:20240120084858+00'00'\", 'keywords': '', 'modDate': \"D:20240120084858+00'00'\", 'creationdate': '2024-01-20T08:48:58+00:00', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'title': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'trapped': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'total_pages': 6, 'page': 0, 'format': 'PDF 1.4', 'moddate': '2024-01-20T08:48:58+00:00', 'subject': '', 'producer': 'Skia/PDF m120', 'author': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='a1c9c09a-fdb9-412c-b0dc-4b4be9a16ce3', metadata={'keywords': '', 'trapped': '', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'format': 'PDF 1.4', 'author': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'modDate': \"D:20240120084858+00'00'\", 'subject': '', 'moddate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'creationDate': \"D:20240120084858+00'00'\", 'total_pages': 6, 'producer': 'Skia/PDF m120', 'page': 5, 'title': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf'}, page_content='This sheds light on the obvious disadvantage of ridge regression, which is\\nmodel interpretability. It will shrink the coefficients for least important\\npredictors, very close to zero. But it will never make them exactly zero. In\\nother words, the final model will include all predictors. However, in the case\\nof the lasso, the L1 penalty has the eÔ¨Äect of forcing some of the coeÔ¨Écient\\nestimates to be exactly equal to zero when the tuning parameter Œª is\\nsuÔ¨Éciently large. Therefore, the lasso method also performs variable\\nselection and is said to yield sparse models.\\nWhat does Regularization achieve?\\nA standard least squares model tends to have some variance in it, i.e. this\\nmodel won‚Äôt generalize well for a data set different than its training data.\\nRegularization, significantly reduces the variance of the model, without\\nsubstantial increase in its bias. So the tuning parameter Œª, used in the\\nregularization techniques described above, controls the impact on bias and\\nvariance. As the value of Œª rises, it reduces the value of coefficients and thus\\nreducing the variance. Till a point, this increase in Œª is beneficial as it is only\\nreducing the variance(hence avoiding overfitting), without loosing any\\nimportant properties in the data. But after certain value, the model starts\\nloosing important properties, giving rise to bias in the model and thus\\nunderfitting. Therefore, the value of Œª should be carefully selected.\\nThis is all the basic you will need, to get started with Regularization. It is a\\nuseful technique that can help in improving the accuracy of your regression\\nmodels. A popular library for implementing these algorithms is Scikit-\\nLearn. It has a wonderful api that can get your model up an running with\\njust a few lines of code in python.\\nIf you liked this article, be sure to show your support by clapping for this\\narticle below and if you have any questions, leave a comment and I will do\\nmy best to answer.'), 0)\n",
      "Content: This sheds light on the obvious disadvantage of ridge regression, which is\n",
      "model interpretability. It will shrink the coefficients for least important\n",
      "predictors, very close to zero. But it will never make them exactly zero. In\n",
      "other words, the final model will include all predictors. However, in the case\n",
      "of the lasso, the L1 penalty has the eÔ¨Äect of forcing some of the coeÔ¨Écient\n",
      "estimates to be exactly equal to zero when the tuning parameter Œª is\n",
      "suÔ¨Éciently large. Therefore, the lasso method also performs variable\n",
      "selection and is said to yield sparse models.\n",
      "What does Regularization achieve?\n",
      "A standard least squares model tends to have some variance in it, i.e. this\n",
      "model won‚Äôt generalize well for a data set different than its training data.\n",
      "Regularization, significantly reduces the variance of the model, without\n",
      "substantial increase in its bias. So the tuning parameter Œª, used in the\n",
      "regularization techniques described above, controls the impact on bias and\n",
      "variance. As the value of Œª rises, it reduces the value of coefficients and thus\n",
      "reducing the variance. Till a point, this increase in Œª is beneficial as it is only\n",
      "reducing the variance(hence avoiding overfitting), without loosing any\n",
      "important properties in the data. But after certain value, the model starts\n",
      "loosing important properties, giving rise to bias in the model and thus\n",
      "underfitting. Therefore, the value of Œª should be carefully selected.\n",
      "This is all the basic you will need, to get started with Regularization. It is a\n",
      "useful technique that can help in improving the accuracy of your regression\n",
      "models. A popular library for implementing these algorithms is Scikit-\n",
      "Learn. It has a wonderful api that can get your model up an running with\n",
      "just a few lines of code in python.\n",
      "If you liked this article, be sure to show your support by clapping for this\n",
      "article below and if you have any questions, leave a comment and I will do\n",
      "my best to answer. \n",
      "\n",
      "Metadata: {'keywords': '', 'trapped': '', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'format': 'PDF 1.4', 'author': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'modDate': \"D:20240120084858+00'00'\", 'subject': '', 'moddate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'creationDate': \"D:20240120084858+00'00'\", 'total_pages': 6, 'producer': 'Skia/PDF m120', 'page': 5, 'title': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='4e552fa7-7f9d-4fe4-81cb-2e85849e8467', metadata={'format': 'PDF 1.4', 'keywords': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'trapped': '', 'title': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'creationdate': '2024-01-27T05:53:28+00:00', 'moddate': '2024-01-27T05:53:28+00:00', 'producer': 'Skia/PDF m120', 'author': '', 'creationDate': \"D:20240127055328+00'00'\", 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'modDate': \"D:20240127055328+00'00'\", 'subject': '', 'total_pages': 8, 'page': 4}, page_content='- Lasso regression (or L1 regularization) is a regularization technique that penalizes high-\\nvalue, correlated coefficients. It introduces a regularization term (also called, penalty\\nterm) into the model‚Äôs sum of squared errors (SSE) loss function. This penalty term is the\\nabsolute value of the sum of coefficients. Controlled in turn by the hyperparameter\\nlambda (Œª), it reduces select feature weights to zero. Lasso regression thereby removes\\nmulticollinear features from the model altogether.\\n- Ridge regression (or L2 regularization) is regularization technique that similarly\\npenalizes high-value coefficients by introducing a penalty term in the SSE loss function. It\\ndiffers from lasso regression however. First, the penalty term in ridge regression is the\\nsquared sum of coefficients rather than the absolute value of coefficients. Second, ridge\\nregression does not enact feature selection. While lasso regression‚Äôs penalty term can\\nremove features from the model by shrinking coefficient values to zero, ridge regression\\nonly shrinks feature weights towards zero but never to zero.\\n- Elastic net regularization essentially combines both ridge and lasso regression but\\ninserting both the L1 and L2 penalty terms into the SSE loss function. L2 and L1 derive\\ntheir penalty term value, respectively, by squaring or taking the absolute value of the sum\\nof the feature weights. Elastic net inserts both of these penalty values into the cost\\nfunction (SSE) equation. In this way, elastic net addresses multicollinearity while also\\nenabling feature selection.6\\nIn statistics, these methods are also dubbed ‚Äúcoefficient shrinkage,‚Äù as they shrink\\npredictor coefficient values in the predictive model. In all three techniques, the strength\\nof the penalty term is controlled by lambda, which can be calculated using various cross-\\nvalidation techniques.\\nTypes of regularization in machine\\nlearning\\nDataset\\nData augmentation is a regularization technique that modifies model training data. It\\nexpands the size of the training set by creating artificial data samples derived from pre-\\nexisting training data. Adding more samples to the training set, particularly of instances\\nrare in real world data, exposes a model to a greater quantity and diversity of data from\\nwhich it learns. Machine learning research has recently explored data augmentation for\\nclassifiers, particularly as a means of resolving imbalanced datasets.7 Data augmentation\\ndiffers from synthetic data however. The latter involves creating new, artificial data while'), 0)\n",
      "Content: - Lasso regression (or L1 regularization) is a regularization technique that penalizes high-\n",
      "value, correlated coefficients. It introduces a regularization term (also called, penalty\n",
      "term) into the model‚Äôs sum of squared errors (SSE) loss function. This penalty term is the\n",
      "absolute value of the sum of coefficients. Controlled in turn by the hyperparameter\n",
      "lambda (Œª), it reduces select feature weights to zero. Lasso regression thereby removes\n",
      "multicollinear features from the model altogether.\n",
      "- Ridge regression (or L2 regularization) is regularization technique that similarly\n",
      "penalizes high-value coefficients by introducing a penalty term in the SSE loss function. It\n",
      "differs from lasso regression however. First, the penalty term in ridge regression is the\n",
      "squared sum of coefficients rather than the absolute value of coefficients. Second, ridge\n",
      "regression does not enact feature selection. While lasso regression‚Äôs penalty term can\n",
      "remove features from the model by shrinking coefficient values to zero, ridge regression\n",
      "only shrinks feature weights towards zero but never to zero.\n",
      "- Elastic net regularization essentially combines both ridge and lasso regression but\n",
      "inserting both the L1 and L2 penalty terms into the SSE loss function. L2 and L1 derive\n",
      "their penalty term value, respectively, by squaring or taking the absolute value of the sum\n",
      "of the feature weights. Elastic net inserts both of these penalty values into the cost\n",
      "function (SSE) equation. In this way, elastic net addresses multicollinearity while also\n",
      "enabling feature selection.6\n",
      "In statistics, these methods are also dubbed ‚Äúcoefficient shrinkage,‚Äù as they shrink\n",
      "predictor coefficient values in the predictive model. In all three techniques, the strength\n",
      "of the penalty term is controlled by lambda, which can be calculated using various cross-\n",
      "validation techniques.\n",
      "Types of regularization in machine\n",
      "learning\n",
      "Dataset\n",
      "Data augmentation is a regularization technique that modifies model training data. It\n",
      "expands the size of the training set by creating artificial data samples derived from pre-\n",
      "existing training data. Adding more samples to the training set, particularly of instances\n",
      "rare in real world data, exposes a model to a greater quantity and diversity of data from\n",
      "which it learns. Machine learning research has recently explored data augmentation for\n",
      "classifiers, particularly as a means of resolving imbalanced datasets.7 Data augmentation\n",
      "differs from synthetic data however. The latter involves creating new, artificial data while \n",
      "\n",
      "Metadata: {'format': 'PDF 1.4', 'keywords': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'trapped': '', 'title': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'creationdate': '2024-01-27T05:53:28+00:00', 'moddate': '2024-01-27T05:53:28+00:00', 'producer': 'Skia/PDF m120', 'author': '', 'creationDate': \"D:20240127055328+00'00'\", 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'modDate': \"D:20240127055328+00'00'\", 'subject': '', 'total_pages': 8, 'page': 4}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='fe514249-1eed-4ad2-b81a-7b0bcef9c677', metadata={'title': '', 'keywords': '', 'subject': '', 'creationDate': \"D:20240127055328+00'00'\", 'producer': 'Skia/PDF m120', 'author': '', 'page': 5, 'trapped': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'total_pages': 8, 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'moddate': '2024-01-27T05:53:28+00:00', 'creationdate': '2024-01-27T05:53:28+00:00'}, page_content='the former produces modified duplicates of preexisting data to diversify and enlarge the\\ndataset.\\nModel training\\nEarly stopping is perhaps the most readily implemented regularization technique. In\\nshort, it limits the number of iterations during model training. Here, a model continuously\\npasses through the training data, stopping once there is no improvement (and perhaps\\neven deterioration) in training and validation accuracy. The goal is to train a model until it\\nhas reached the lowest possible training error preceding a plateau or increase in\\nvalidation error.8\\nMany machine learning Python packages provide a training command options for early\\nstopping. In fact, in some, early stopping is a default training setting.'), 0)\n",
      "Content: the former produces modified duplicates of preexisting data to diversify and enlarge the\n",
      "dataset.\n",
      "Model training\n",
      "Early stopping is perhaps the most readily implemented regularization technique. In\n",
      "short, it limits the number of iterations during model training. Here, a model continuously\n",
      "passes through the training data, stopping once there is no improvement (and perhaps\n",
      "even deterioration) in training and validation accuracy. The goal is to train a model until it\n",
      "has reached the lowest possible training error preceding a plateau or increase in\n",
      "validation error.8\n",
      "Many machine learning Python packages provide a training command options for early\n",
      "stopping. In fact, in some, early stopping is a default training setting. \n",
      "\n",
      "Metadata: {'title': '', 'keywords': '', 'subject': '', 'creationDate': \"D:20240127055328+00'00'\", 'producer': 'Skia/PDF m120', 'author': '', 'page': 5, 'trapped': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'total_pages': 8, 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'moddate': '2024-01-27T05:53:28+00:00', 'creationdate': '2024-01-27T05:53:28+00:00'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='8e7d256e-99b0-47ad-acb7-ade24ce9ea37', metadata={'title': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'author': '', 'trapped': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'moddate': '2024-01-27T05:53:28+00:00', 'subject': '', 'format': 'PDF 1.4', 'page': 3, 'creationDate': \"D:20240127055328+00'00'\", 'keywords': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'creationdate': '2024-01-27T05:53:28+00:00', 'total_pages': 8, 'producer': 'Skia/PDF m120'}, page_content='Related content\\nTypes of regularization with linear\\nmodels\\nLinear regression and logistic regression are both predictive models underpinning\\nmachine learning. Linear regression (or ordinary least squares) aims to measure and\\npredict the impact of one or more predictors on a given output by finding the best fitting\\nline through provided data points (i.e. training data). Logistic regression aims to\\ndetermine the class probabilities of by way of a binary output given a range of predictors.\\nIn other words, linear regression makes continuous quantitative predictions while logistic\\nregression produces discreet categorical predictions.5\\nOf course, as the number of predictors increase in either regression model, the input-\\noutput relationship is not always straightforward and requires manipulation of the\\nregression formula. Enter regularization. There are three main forms of regularization for\\nregression models. Note that this list is only a brief survey. Application of these\\nregularization techniques in either linear or logistic regression varies minutely.\\nDemo\\nTake a tour of IBM watsonx\\nExplore IBM watsonx and learn how to create machine learning models using statistical\\ndatasets\\nSubscribe to the IBM newsletter'), 0)\n",
      "Content: Related content\n",
      "Types of regularization with linear\n",
      "models\n",
      "Linear regression and logistic regression are both predictive models underpinning\n",
      "machine learning. Linear regression (or ordinary least squares) aims to measure and\n",
      "predict the impact of one or more predictors on a given output by finding the best fitting\n",
      "line through provided data points (i.e. training data). Logistic regression aims to\n",
      "determine the class probabilities of by way of a binary output given a range of predictors.\n",
      "In other words, linear regression makes continuous quantitative predictions while logistic\n",
      "regression produces discreet categorical predictions.5\n",
      "Of course, as the number of predictors increase in either regression model, the input-\n",
      "output relationship is not always straightforward and requires manipulation of the\n",
      "regression formula. Enter regularization. There are three main forms of regularization for\n",
      "regression models. Note that this list is only a brief survey. Application of these\n",
      "regularization techniques in either linear or logistic regression varies minutely.\n",
      "Demo\n",
      "Take a tour of IBM watsonx\n",
      "Explore IBM watsonx and learn how to create machine learning models using statistical\n",
      "datasets\n",
      "Subscribe to the IBM newsletter \n",
      "\n",
      "Metadata: {'title': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'author': '', 'trapped': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'moddate': '2024-01-27T05:53:28+00:00', 'subject': '', 'format': 'PDF 1.4', 'page': 3, 'creationDate': \"D:20240127055328+00'00'\", 'keywords': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'creationdate': '2024-01-27T05:53:28+00:00', 'total_pages': 8, 'producer': 'Skia/PDF m120'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='c78bb6b8-41e8-41f9-a236-73ebd20fbaec', metadata={'creationDate': \"D:20240127055328+00'00'\", 'subject': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'total_pages': 8, 'modDate': \"D:20240127055328+00'00'\", 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'producer': 'Skia/PDF m120', 'moddate': '2024-01-27T05:53:28+00:00', 'title': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'keywords': '', 'author': '', 'page': 7, 'trapped': '', 'format': 'PDF 1.4', 'creationdate': '2024-01-27T05:53:28+00:00'}, page_content='Weight decay is another form of regularization used for deep neural networks. It reduces\\nthe sum of squared network weights by way of a regularization parameter, much like L2\\nregularization in linear models.10 But when employed in neural networks, this reduction\\nhas an effect similar to L1 regularization: select neuron weights decrease to zero.11 This\\neffectively removes nodes from the network, reducing network complexity through\\nsparsity.12\\nWeight decay may appear superficially similar to dropout in deep neural networks, but\\nthe two techniques differ. One primary difference is that, in dropout, the penalty value\\ngrows exponentially in the network‚Äôs depth in cases, whereas weight decay‚Äôs penalty\\nvalue grows linearly. Some believe this allows dropout to more meaningfully penalize\\nnetwork complexity than weight decay.13\\nMany online articles and tutorials incorrectly conflate L2 regularization and weight decay.\\nIn fact, scholarship is inconsistent‚Äîsome distinguish between L2 and weight decay,14\\nsome equate them,15 while others are inconsistent in describing the relationship between\\nthem.16 Resolving such inconsistencies in terminology is a needed yet overlooked area for\\nfuture scholarship.'), 0)\n",
      "Content: Weight decay is another form of regularization used for deep neural networks. It reduces\n",
      "the sum of squared network weights by way of a regularization parameter, much like L2\n",
      "regularization in linear models.10 But when employed in neural networks, this reduction\n",
      "has an effect similar to L1 regularization: select neuron weights decrease to zero.11 This\n",
      "effectively removes nodes from the network, reducing network complexity through\n",
      "sparsity.12\n",
      "Weight decay may appear superficially similar to dropout in deep neural networks, but\n",
      "the two techniques differ. One primary difference is that, in dropout, the penalty value\n",
      "grows exponentially in the network‚Äôs depth in cases, whereas weight decay‚Äôs penalty\n",
      "value grows linearly. Some believe this allows dropout to more meaningfully penalize\n",
      "network complexity than weight decay.13\n",
      "Many online articles and tutorials incorrectly conflate L2 regularization and weight decay.\n",
      "In fact, scholarship is inconsistent‚Äîsome distinguish between L2 and weight decay,14\n",
      "some equate them,15 while others are inconsistent in describing the relationship between\n",
      "them.16 Resolving such inconsistencies in terminology is a needed yet overlooked area for\n",
      "future scholarship. \n",
      "\n",
      "Metadata: {'creationDate': \"D:20240127055328+00'00'\", 'subject': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'total_pages': 8, 'modDate': \"D:20240127055328+00'00'\", 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'producer': 'Skia/PDF m120', 'moddate': '2024-01-27T05:53:28+00:00', 'title': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'keywords': '', 'author': '', 'page': 7, 'trapped': '', 'format': 'PDF 1.4', 'creationdate': '2024-01-27T05:53:28+00:00'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='a690149c-2388-48c8-b70c-a9ed0cf673b1', metadata={'subject': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'producer': 'Skia/PDF m120', 'modDate': \"D:20240120084858+00'00'\", 'format': 'PDF 1.4', 'moddate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'total_pages': 6, 'author': '', 'title': '', 'keywords': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'page': 1, 'trapped': '', 'creationDate': \"D:20240120084858+00'00'\"}, page_content='The concept of balancing bias and variance, is helpful in understanding the\\nphenomenon of overfitting.\\nBalancing Bias and Variance to Control Errors in Machine\\nLearning\\nIn the world of Machine Learning, accuracy is everything. You strive\\nto make your model more accurate by tuning and‚Ä¶\\nmedium.com\\nOne of the ways of avoiding overfitting is using cross validation, that helps in\\nestimating the error over test set, and in deciding what parameters work best for\\nyour model.\\nCross-Validation in Machine Learning\\nThere is always a need to validate the stability of your machine\\nlearning model. I mean you just can‚Äôt fit the model to‚Ä¶\\nmedium.com\\nThis article will focus on a technique that helps in avoiding overfitting and\\nalso increasing model interpretability.\\nRegularization\\nThis is a form of regression, that constrains/ regularizes or shrinks the\\ncoefficient estimates towards zero. In other words, this technique discourages\\nlearning a more complex or flexible model, so as to avoid the risk of overfitting.\\nA simple relation for linear regression looks like this. Here Y represents the\\nlearned relation and Œ≤ represents the coefficient estimates for different variables\\nor predictors(X).\\nY ‚âà Œ≤0 + Œ≤1X1 + Œ≤2X2 + ‚Ä¶+ Œ≤pXp\\nThe fitting procedure involves a loss function, known as residual sum of\\nsquares or RSS. The coefficients are chosen, such that they minimize this'), 0)\n",
      "Content: The concept of balancing bias and variance, is helpful in understanding the\n",
      "phenomenon of overfitting.\n",
      "Balancing Bias and Variance to Control Errors in Machine\n",
      "Learning\n",
      "In the world of Machine Learning, accuracy is everything. You strive\n",
      "to make your model more accurate by tuning and‚Ä¶\n",
      "medium.com\n",
      "One of the ways of avoiding overfitting is using cross validation, that helps in\n",
      "estimating the error over test set, and in deciding what parameters work best for\n",
      "your model.\n",
      "Cross-Validation in Machine Learning\n",
      "There is always a need to validate the stability of your machine\n",
      "learning model. I mean you just can‚Äôt fit the model to‚Ä¶\n",
      "medium.com\n",
      "This article will focus on a technique that helps in avoiding overfitting and\n",
      "also increasing model interpretability.\n",
      "Regularization\n",
      "This is a form of regression, that constrains/ regularizes or shrinks the\n",
      "coefficient estimates towards zero. In other words, this technique discourages\n",
      "learning a more complex or flexible model, so as to avoid the risk of overfitting.\n",
      "A simple relation for linear regression looks like this. Here Y represents the\n",
      "learned relation and Œ≤ represents the coefficient estimates for different variables\n",
      "or predictors(X).\n",
      "Y ‚âà Œ≤0 + Œ≤1X1 + Œ≤2X2 + ‚Ä¶+ Œ≤pXp\n",
      "The fitting procedure involves a loss function, known as residual sum of\n",
      "squares or RSS. The coefficients are chosen, such that they minimize this \n",
      "\n",
      "Metadata: {'subject': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'producer': 'Skia/PDF m120', 'modDate': \"D:20240120084858+00'00'\", 'format': 'PDF 1.4', 'moddate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'total_pages': 6, 'author': '', 'title': '', 'keywords': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'page': 1, 'trapped': '', 'creationDate': \"D:20240120084858+00'00'\"}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='af03907b-9eb8-4c0c-afe0-da8ce53ea776', metadata={'trapped': '', 'modDate': \"D:20240120084858+00'00'\", 'author': '', 'total_pages': 6, 'keywords': '', 'moddate': '2024-01-20T08:48:58+00:00', 'subject': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'page': 2, 'producer': 'Skia/PDF m120', 'creationDate': \"D:20240120084858+00'00'\", 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'title': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf'}, page_content='loss function.\\nNow, this will adjust the coefficients based on your training data. If there is\\nnoise in the training data, then the estimated coefficients won‚Äôt generalize well to\\nthe future data. This is where regularization comes in and shrinks or regularizes\\nthese learned estimates towards zero.\\nRidge Regression\\nAbove image shows ridge regression, where the RSS is modified by adding the\\nshrinkage quantity. Now, the coefficients are estimated by minimizing this\\nfunction. Here, Œª is the tuning parameter that decides how much we want to\\npenalize the flexibility of our model. The increase in flexibility of a model is\\nrepresented by increase in its coefficients, and if we want to minimize the\\nabove function, then these coefficients need to be small. This is how the\\nRidge regression technique prevents coefficients from rising too high. Also,\\nnotice that we shrink the estimated association of each variable with the\\nresponse, except the intercept Œ≤0, This intercept is a measure of the mean\\nvalue of the response when xi1 = xi2 = ‚Ä¶= xip = 0.\\nWhen Œª = 0, the penalty term has no eÔ¨Äect, and the estimates produced by ridge\\nregression will be equal to least squares. However, as Œª‚Üí‚àû, the impact of the\\nshrinkage penalty grows, and the ridge regression coeÔ¨Écient estimates will\\napproach zero. As can be seen, selecting a good value of Œª is critical. Cross\\nvalidation comes in handy for this purpose. The coefficient estimates\\nproduced by this method are also known as the L2 norm.\\nOpen in app\\nSign up\\nSign in\\nSearch\\nWrite'), 0)\n",
      "Content: loss function.\n",
      "Now, this will adjust the coefficients based on your training data. If there is\n",
      "noise in the training data, then the estimated coefficients won‚Äôt generalize well to\n",
      "the future data. This is where regularization comes in and shrinks or regularizes\n",
      "these learned estimates towards zero.\n",
      "Ridge Regression\n",
      "Above image shows ridge regression, where the RSS is modified by adding the\n",
      "shrinkage quantity. Now, the coefficients are estimated by minimizing this\n",
      "function. Here, Œª is the tuning parameter that decides how much we want to\n",
      "penalize the flexibility of our model. The increase in flexibility of a model is\n",
      "represented by increase in its coefficients, and if we want to minimize the\n",
      "above function, then these coefficients need to be small. This is how the\n",
      "Ridge regression technique prevents coefficients from rising too high. Also,\n",
      "notice that we shrink the estimated association of each variable with the\n",
      "response, except the intercept Œ≤0, This intercept is a measure of the mean\n",
      "value of the response when xi1 = xi2 = ‚Ä¶= xip = 0.\n",
      "When Œª = 0, the penalty term has no eÔ¨Äect, and the estimates produced by ridge\n",
      "regression will be equal to least squares. However, as Œª‚Üí‚àû, the impact of the\n",
      "shrinkage penalty grows, and the ridge regression coeÔ¨Écient estimates will\n",
      "approach zero. As can be seen, selecting a good value of Œª is critical. Cross\n",
      "validation comes in handy for this purpose. The coefficient estimates\n",
      "produced by this method are also known as the L2 norm.\n",
      "Open in app\n",
      "Sign up\n",
      "Sign in\n",
      "Search\n",
      "Write \n",
      "\n",
      "Metadata: {'trapped': '', 'modDate': \"D:20240120084858+00'00'\", 'author': '', 'total_pages': 6, 'keywords': '', 'moddate': '2024-01-20T08:48:58+00:00', 'subject': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'page': 2, 'producer': 'Skia/PDF m120', 'creationDate': \"D:20240120084858+00'00'\", 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'title': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='4acfcbb9-35b7-4e5d-8e0b-68a5c607a371', metadata={'title': '', 'total_pages': 5, 'creationdate': '2024-01-27T06:07:53+00:00', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'moddate': '2024-01-27T06:07:53+00:00', 'format': 'PDF 1.4', 'trapped': '', 'creationDate': \"D:20240127060753+00'00'\", 'page': 3, 'producer': 'Skia/PDF m120', 'author': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'keywords': '', 'subject': '', 'modDate': \"D:20240127060753+00'00'\"}, page_content='Loss = f(preds, y) + l*abs(W)\\nL2 Regularization\\nL2 regularization is very similar to L1 regularization, except the penalty term\\nis the square of the parameters scaled by some factor l (lambda)\\nLoss = f(preds, y) + l*(W)¬≤\\nDifference between L1 and L2 regularization\\nThe difference between L1 and L2 regularization is that the gradients of the\\nloss function with respect to parameters for L1 regularization are\\nINDEPENDENT of parameters, so some parameters can be set all the way to\\nzero, hence completely ignored.\\nBut in L2 regularization, the gradients of the loss function are DEPENDENT\\nlinearly on the parameters, so the parameters can never be zero. They only\\nasymptotically approach zero. This means that no parameter is entirely\\nignored, and every parameter always has at least a very minimal effect on\\npredictions.\\nThis difference is key to choosing the type of regularization, if you know you\\nhave useless features, L1 might be a better choice. If you want to consider all\\nfeatures, the L2 might be a better choice.\\nNuance: There is one nuance in deep learning where you can use a best of both\\nworlds approach, by using both L1 and L2 regularization. This is called Elastic net\\nRegularization.\\nDropout\\nDropout is an amazing regularization technique that works only on neural\\nnetworks (as far as I know). The amazing idea of dropout is to randomly zero'), 0)\n",
      "Content: Loss = f(preds, y) + l*abs(W)\n",
      "L2 Regularization\n",
      "L2 regularization is very similar to L1 regularization, except the penalty term\n",
      "is the square of the parameters scaled by some factor l (lambda)\n",
      "Loss = f(preds, y) + l*(W)¬≤\n",
      "Difference between L1 and L2 regularization\n",
      "The difference between L1 and L2 regularization is that the gradients of the\n",
      "loss function with respect to parameters for L1 regularization are\n",
      "INDEPENDENT of parameters, so some parameters can be set all the way to\n",
      "zero, hence completely ignored.\n",
      "But in L2 regularization, the gradients of the loss function are DEPENDENT\n",
      "linearly on the parameters, so the parameters can never be zero. They only\n",
      "asymptotically approach zero. This means that no parameter is entirely\n",
      "ignored, and every parameter always has at least a very minimal effect on\n",
      "predictions.\n",
      "This difference is key to choosing the type of regularization, if you know you\n",
      "have useless features, L1 might be a better choice. If you want to consider all\n",
      "features, the L2 might be a better choice.\n",
      "Nuance: There is one nuance in deep learning where you can use a best of both\n",
      "worlds approach, by using both L1 and L2 regularization. This is called Elastic net\n",
      "Regularization.\n",
      "Dropout\n",
      "Dropout is an amazing regularization technique that works only on neural\n",
      "networks (as far as I know). The amazing idea of dropout is to randomly zero \n",
      "\n",
      "Metadata: {'title': '', 'total_pages': 5, 'creationdate': '2024-01-27T06:07:53+00:00', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'moddate': '2024-01-27T06:07:53+00:00', 'format': 'PDF 1.4', 'trapped': '', 'creationDate': \"D:20240127060753+00'00'\", 'page': 3, 'producer': 'Skia/PDF m120', 'author': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'keywords': '', 'subject': '', 'modDate': \"D:20240127060753+00'00'\"}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='65f3101f-cec1-4bea-8c4a-b43126f60649', metadata={'subject': '', 'format': 'PDF 1.4', 'producer': 'Skia/PDF m120', 'modDate': \"D:20240127055328+00'00'\", 'author': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'moddate': '2024-01-27T05:53:28+00:00', 'keywords': '', 'trapped': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'title': '', 'page': 2, 'total_pages': 8, 'creationDate': \"D:20240127055328+00'00'\", 'creationdate': '2024-01-27T05:53:28+00:00', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf'}, page_content='This concession of increased training error for decreased testing error is known as bias-\\nvariance trade-off. Bias-variance tradeoff is a well-known problem in machine learning.\\nIt‚Äôs necessary to first define ‚Äúbias‚Äù and ‚Äúvariance.‚Äù To put it briefly:\\n- Bias measures the average difference between predicted values and true values. As\\nbias increases, a model predicts less accurately on a training dataset. High bias refers to\\nhigh error in training.\\n- Variance measures the difference between predictions across various realizations of a\\ngiven model. As variance increases, a model predicts less accurately on unseen data.\\nHigh variance refers to high error during testing and validation.\\nBias and variance thus inversely represent model accuracy on training and test sets\\nrespectively.2 Obviously, developers aim to reduce both model bias and variance.\\nSimultaneous reduction in both is not always possible, resulting in the need for\\nregularization. Regularization decreases model variance at the cost of increased bias.\\nRegression model fits\\nBy increasing bias and decreasing variance, regularization resolves model overfitting.\\nOverfitting occurs when error on training data decreases while error on testing data\\nceases decreasing or begins increasing.3 In other words, overfitting describes models\\nwith low bias and high variance. However, if regularization introduces too much bias, then\\na model will underfit.\\nDespite its name, underfitting does not denote overfitting‚Äôs opposite. Rather underfitting\\ndescribes models characterized by high bias and high variance. An underfitted model\\nproduces unsatisfactorily erroneous predictions during training and testing. This often\\nresults from insufficient training data or parameters.\\nRegularization, however, can potentially lead to model underfitting as well. If too much\\nbias is introduced through regularization, model variance can cease to decrease and even\\nincrease. Regularization may have this effect particularly on simple models, i.e. models\\nwith few parameters. In determining the type and degree of regularization to implement,\\nthen, one must consider a model‚Äôs complexity, dataset, and so forth.4'), 0)\n",
      "Content: This concession of increased training error for decreased testing error is known as bias-\n",
      "variance trade-off. Bias-variance tradeoff is a well-known problem in machine learning.\n",
      "It‚Äôs necessary to first define ‚Äúbias‚Äù and ‚Äúvariance.‚Äù To put it briefly:\n",
      "- Bias measures the average difference between predicted values and true values. As\n",
      "bias increases, a model predicts less accurately on a training dataset. High bias refers to\n",
      "high error in training.\n",
      "- Variance measures the difference between predictions across various realizations of a\n",
      "given model. As variance increases, a model predicts less accurately on unseen data.\n",
      "High variance refers to high error during testing and validation.\n",
      "Bias and variance thus inversely represent model accuracy on training and test sets\n",
      "respectively.2 Obviously, developers aim to reduce both model bias and variance.\n",
      "Simultaneous reduction in both is not always possible, resulting in the need for\n",
      "regularization. Regularization decreases model variance at the cost of increased bias.\n",
      "Regression model fits\n",
      "By increasing bias and decreasing variance, regularization resolves model overfitting.\n",
      "Overfitting occurs when error on training data decreases while error on testing data\n",
      "ceases decreasing or begins increasing.3 In other words, overfitting describes models\n",
      "with low bias and high variance. However, if regularization introduces too much bias, then\n",
      "a model will underfit.\n",
      "Despite its name, underfitting does not denote overfitting‚Äôs opposite. Rather underfitting\n",
      "describes models characterized by high bias and high variance. An underfitted model\n",
      "produces unsatisfactorily erroneous predictions during training and testing. This often\n",
      "results from insufficient training data or parameters.\n",
      "Regularization, however, can potentially lead to model underfitting as well. If too much\n",
      "bias is introduced through regularization, model variance can cease to decrease and even\n",
      "increase. Regularization may have this effect particularly on simple models, i.e. models\n",
      "with few parameters. In determining the type and degree of regularization to implement,\n",
      "then, one must consider a model‚Äôs complexity, dataset, and so forth.4 \n",
      "\n",
      "Metadata: {'subject': '', 'format': 'PDF 1.4', 'producer': 'Skia/PDF m120', 'modDate': \"D:20240127055328+00'00'\", 'author': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'moddate': '2024-01-27T05:53:28+00:00', 'keywords': '', 'trapped': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'title': '', 'page': 2, 'total_pages': 8, 'creationDate': \"D:20240127055328+00'00'\", 'creationdate': '2024-01-27T05:53:28+00:00', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='54ec0626-8787-4cf1-af2f-51192c39bc26', metadata={'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'author': '', 'trapped': '', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'page': 4, 'keywords': '', 'creationdate': '2024-01-27T06:07:53+00:00', 'moddate': '2024-01-27T06:07:53+00:00', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'producer': 'Skia/PDF m120', 'total_pages': 5, 'subject': '', 'format': 'PDF 1.4', 'creationDate': \"D:20240127060753+00'00'\", 'title': '', 'modDate': \"D:20240127060753+00'00'\"}, page_content='some elements of the input tensor with probability p (p is a\\nhyperparameter).\\nThe intuition behind why this would work is simple, overfitting occurs when\\nour model is too complex, so how can we simplify the model? Just don‚Äôt use\\nsome neurons and BAM!! a simpler model achieved.\\nDropout is found to work very well in practice and is simple to implement. I\\ndefinitely recommend giving it a try.\\nData Augmentation\\nData augmentation is our final ‚Äúhow‚Äù for regularization. The idea behind\\ndata augmentation is very simple, yet extremely elegant.\\nWe know overfitting is caused by a lack of constraint in the search space of\\nour optimal function. How do we add more constraint? More data.\\nBut collecting more data for our problem can be a time consuming and\\narduous task. This is where data augmentation comes in.\\nThe idea of data augmentation is to create more data from the data we\\nalready have. The way this is done is by applying a few transformations to\\nour image, that change the image to make multiple versions of the same\\nimage that are different. And magically (NOT) we have more data.\\nThe list of data augmentations my favorite deep learning library (fastai)\\nprovides can be seen here.\\nConclusion\\nCheck if your model overfits, use one of the above regularization methods to\\nsave it from overfitting. ALWAYS USE DATA AUGMENTATION!! Fin.\\nDeep Learning\\nOptimization\\nAI\\nMachine Learning\\nData Science'), 0)\n",
      "Content: some elements of the input tensor with probability p (p is a\n",
      "hyperparameter).\n",
      "The intuition behind why this would work is simple, overfitting occurs when\n",
      "our model is too complex, so how can we simplify the model? Just don‚Äôt use\n",
      "some neurons and BAM!! a simpler model achieved.\n",
      "Dropout is found to work very well in practice and is simple to implement. I\n",
      "definitely recommend giving it a try.\n",
      "Data Augmentation\n",
      "Data augmentation is our final ‚Äúhow‚Äù for regularization. The idea behind\n",
      "data augmentation is very simple, yet extremely elegant.\n",
      "We know overfitting is caused by a lack of constraint in the search space of\n",
      "our optimal function. How do we add more constraint? More data.\n",
      "But collecting more data for our problem can be a time consuming and\n",
      "arduous task. This is where data augmentation comes in.\n",
      "The idea of data augmentation is to create more data from the data we\n",
      "already have. The way this is done is by applying a few transformations to\n",
      "our image, that change the image to make multiple versions of the same\n",
      "image that are different. And magically (NOT) we have more data.\n",
      "The list of data augmentations my favorite deep learning library (fastai)\n",
      "provides can be seen here.\n",
      "Conclusion\n",
      "Check if your model overfits, use one of the above regularization methods to\n",
      "save it from overfitting. ALWAYS USE DATA AUGMENTATION!! Fin.\n",
      "Deep Learning\n",
      "Optimization\n",
      "AI\n",
      "Machine Learning\n",
      "Data Science \n",
      "\n",
      "Metadata: {'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'author': '', 'trapped': '', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'page': 4, 'keywords': '', 'creationdate': '2024-01-27T06:07:53+00:00', 'moddate': '2024-01-27T06:07:53+00:00', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'producer': 'Skia/PDF m120', 'total_pages': 5, 'subject': '', 'format': 'PDF 1.4', 'creationDate': \"D:20240127060753+00'00'\", 'title': '', 'modDate': \"D:20240127060753+00'00'\"}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='4ed0050f-a7d4-42c8-a807-152830f499e2', metadata={'author': '', 'subject': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20240120084858+00'00'\", 'total_pages': 6, 'producer': 'Skia/PDF m120', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'creationDate': \"D:20240120084858+00'00'\", 'trapped': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'page': 3, 'title': '', 'moddate': '2024-01-20T08:48:58+00:00'}, page_content='The coefficients that are produced by the standard least squares method are\\nscale equivariant, i.e. if we multiply each input by c then the corresponding\\ncoefficients are scaled by a factor of 1/c. Therefore, regardless of how the\\npredictor is scaled, the multiplication of predictor and coefficient(XjŒ≤j)\\nremains the same. However, this is not the case with ridge regression, and\\ntherefore, we need to standardize the predictors or bring the predictors to the\\nsame scale before performing ridge regression. The formula used to do this is\\ngiven below.\\nLasso\\nLasso is another variation, in which the above function is minimized. Its\\nclear that this variation differs from ridge regression only in penalizing the\\nhigh coefficients. It uses |Œ≤j|(modulus)instead of squares of Œ≤, as its penalty.\\nIn statistics, this is known as the L1 norm.\\nLets take a look at above methods with a different perspective. The ridge\\nregression can be thought of as solving an equation, where summation of squares\\nof coefficients is less than or equal to s. And the Lasso can be thought of as an\\nequation where summation of modulus of coefficients is less than or equal to s.\\nHere, s is a constant that exists for each value of shrinkage factor Œª. These\\nequations are also referred to as constraint functions.\\nConsider their are 2 parameters in a given problem. Then according to above\\nformulation, the ridge regression is expressed by Œ≤1¬≤ + Œ≤2¬≤ ‚â§ s. This implies\\nthat ridge regression coefficients have the smallest RSS(loss function) for all points\\nthat lie within the circle given by Œ≤1¬≤ + Œ≤2¬≤ ‚â§ s.'), 0)\n",
      "Content: The coefficients that are produced by the standard least squares method are\n",
      "scale equivariant, i.e. if we multiply each input by c then the corresponding\n",
      "coefficients are scaled by a factor of 1/c. Therefore, regardless of how the\n",
      "predictor is scaled, the multiplication of predictor and coefficient(XjŒ≤j)\n",
      "remains the same. However, this is not the case with ridge regression, and\n",
      "therefore, we need to standardize the predictors or bring the predictors to the\n",
      "same scale before performing ridge regression. The formula used to do this is\n",
      "given below.\n",
      "Lasso\n",
      "Lasso is another variation, in which the above function is minimized. Its\n",
      "clear that this variation differs from ridge regression only in penalizing the\n",
      "high coefficients. It uses |Œ≤j|(modulus)instead of squares of Œ≤, as its penalty.\n",
      "In statistics, this is known as the L1 norm.\n",
      "Lets take a look at above methods with a different perspective. The ridge\n",
      "regression can be thought of as solving an equation, where summation of squares\n",
      "of coefficients is less than or equal to s. And the Lasso can be thought of as an\n",
      "equation where summation of modulus of coefficients is less than or equal to s.\n",
      "Here, s is a constant that exists for each value of shrinkage factor Œª. These\n",
      "equations are also referred to as constraint functions.\n",
      "Consider their are 2 parameters in a given problem. Then according to above\n",
      "formulation, the ridge regression is expressed by Œ≤1¬≤ + Œ≤2¬≤ ‚â§ s. This implies\n",
      "that ridge regression coefficients have the smallest RSS(loss function) for all points\n",
      "that lie within the circle given by Œ≤1¬≤ + Œ≤2¬≤ ‚â§ s. \n",
      "\n",
      "Metadata: {'author': '', 'subject': '', 'creationdate': '2024-01-20T08:48:58+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20240120084858+00'00'\", 'total_pages': 6, 'producer': 'Skia/PDF m120', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'creationDate': \"D:20240120084858+00'00'\", 'trapped': '', 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'page': 3, 'title': '', 'moddate': '2024-01-20T08:48:58+00:00'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='e2e98f0d-dc08-4d1d-9f7d-09f0db0d6a48', metadata={'title': '', 'file_path': 'kb_ip\\\\Transformers from Scratch.pdf', 'format': 'PDF 1.4', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0', 'modDate': \"D:20240224103951+00'00'\", 'creationdate': '2024-02-24T10:39:51+00:00', 'author': '', 'moddate': '2024-02-24T10:39:51+00:00', 'subject': '', 'source': 'kb_ip\\\\Transformers from Scratch.pdf', 'creationDate': \"D:20240224103951+00'00'\", 'total_pages': 30, 'page': 25, 'trapped': '', 'producer': 'Skia/PDF m121', 'keywords': ''}, page_content=\"ensure that small changes in any of the inputs will still have noticeable changes in the result. This keeps gradient descent from getting stuck far away\\nfrom a good solution.\\nSkip connections have become popular because of how they improve performance since the days of the ResNet image classifier. They are now a\\nstandard feature in neural network architectures. Visually, we can see the effect that skip connections have by comparing networks with and without\\nthem. The figure below from this paper shows a ResNet with and without skip connections. The slopes of the loss function hills are are much more\\nmoderate and uniform when skip connections are used. If you feel like taking a deeper dive into how the work and why, there's a more in-depth\\ntreatment in this post.\\nThe second purpose of skip connections is specific to transformers ‚Äî preserving the original input sequence. Even with a lot of attention heads, there‚Äôs\\nno guarantee that a word will attend to its own position. It‚Äôs possible for the attention filter to forget entirely about the most recent word in favor of\\nwatching all of the earlier words that might be relevant. A skip connection takes the original word and manually adds it back into the signal, so that\\nthere‚Äôs no way it can be dropped or forgotten. This source of robustness may be one of the reasons for transformers' good behavior in so many varied\\nsequence completion tasks.\\nLayer normalization\\nNormalization is a step that pairs well with skip connections. There's no reason they necessarily have to go together, but they both do their best work\\nwhen placed after a group of calculations, like attention or a feed forward neural network.\\nThe short version of layer normalization is that the values of the matrix are shifted to have a mean of zero and scaled to have a standard deviation of\\none.\\nThe longer version is that in systems like transformers, where there are a lot of moving pieces and some of them are something other than matrix\\nmultiplications (such as softmax operators or rectified linear units), it matters how big values are and how they're balanced between positive and\\nnegative. If everything is linear, you can double all your inputs, and your outputs will be twice as big, and everything will work just fine. Not so with\\nneural networks. They are inherently nonlinear, which makes them very expressive but also sensitive to signals' magnitudes and distributions.\\nNormalization is a technique that has proven useful in maintaining a consistent distribution of signal values each step of the way throughout many-\\nlayered neural networks. It encourages convergence of parameter values and usually results in much better performance.\"), 0)\n",
      "Content: ensure that small changes in any of the inputs will still have noticeable changes in the result. This keeps gradient descent from getting stuck far away\n",
      "from a good solution.\n",
      "Skip connections have become popular because of how they improve performance since the days of the ResNet image classifier. They are now a\n",
      "standard feature in neural network architectures. Visually, we can see the effect that skip connections have by comparing networks with and without\n",
      "them. The figure below from this paper shows a ResNet with and without skip connections. The slopes of the loss function hills are are much more\n",
      "moderate and uniform when skip connections are used. If you feel like taking a deeper dive into how the work and why, there's a more in-depth\n",
      "treatment in this post.\n",
      "The second purpose of skip connections is specific to transformers ‚Äî preserving the original input sequence. Even with a lot of attention heads, there‚Äôs\n",
      "no guarantee that a word will attend to its own position. It‚Äôs possible for the attention filter to forget entirely about the most recent word in favor of\n",
      "watching all of the earlier words that might be relevant. A skip connection takes the original word and manually adds it back into the signal, so that\n",
      "there‚Äôs no way it can be dropped or forgotten. This source of robustness may be one of the reasons for transformers' good behavior in so many varied\n",
      "sequence completion tasks.\n",
      "Layer normalization\n",
      "Normalization is a step that pairs well with skip connections. There's no reason they necessarily have to go together, but they both do their best work\n",
      "when placed after a group of calculations, like attention or a feed forward neural network.\n",
      "The short version of layer normalization is that the values of the matrix are shifted to have a mean of zero and scaled to have a standard deviation of\n",
      "one.\n",
      "The longer version is that in systems like transformers, where there are a lot of moving pieces and some of them are something other than matrix\n",
      "multiplications (such as softmax operators or rectified linear units), it matters how big values are and how they're balanced between positive and\n",
      "negative. If everything is linear, you can double all your inputs, and your outputs will be twice as big, and everything will work just fine. Not so with\n",
      "neural networks. They are inherently nonlinear, which makes them very expressive but also sensitive to signals' magnitudes and distributions.\n",
      "Normalization is a technique that has proven useful in maintaining a consistent distribution of signal values each step of the way throughout many-\n",
      "layered neural networks. It encourages convergence of parameter values and usually results in much better performance. \n",
      "\n",
      "Metadata: {'title': '', 'file_path': 'kb_ip\\\\Transformers from Scratch.pdf', 'format': 'PDF 1.4', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36 Edg/121.0.0.0', 'modDate': \"D:20240224103951+00'00'\", 'creationdate': '2024-02-24T10:39:51+00:00', 'author': '', 'moddate': '2024-02-24T10:39:51+00:00', 'subject': '', 'source': 'kb_ip\\\\Transformers from Scratch.pdf', 'creationDate': \"D:20240224103951+00'00'\", 'total_pages': 30, 'page': 25, 'trapped': '', 'producer': 'Skia/PDF m121', 'keywords': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='a6100c35-ebd1-4529-8468-222b7b5c1574', metadata={'total_pages': 8, 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'creationdate': '2024-01-27T05:53:28+00:00', 'creationDate': \"D:20240127055328+00'00'\", 'format': 'PDF 1.4', 'moddate': '2024-01-27T05:53:28+00:00', 'title': '', 'page': 6, 'subject': '', 'trapped': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'keywords': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'producer': 'Skia/PDF m120', 'author': ''}, page_content='Neural networks\\nNeural networks are complex machine learning models that drive many artificial\\nintelligence applications and services. Neural networks are composed of an input layer,\\none or more hidden layers, and an output layer, each layer in turn comprised of several\\nnodes.\\nDropout regularizes neural networks by randomly dropping out nodes, along with their\\ninput and output connections, from the network during training (Fig. 3). Dropout trains\\nseveral variations of a fixed-sized architecture, with each variation having different\\nrandomized nodes left out of the architecture. A single neural net without dropout is used\\nfor testing, employing an approximate averaging method derived from the randomly\\nmodified training architectures. In this way, dropout approximates training large a\\nquantity of neural networks with a multitude of diversified architectures.9'), 0)\n",
      "Content: Neural networks\n",
      "Neural networks are complex machine learning models that drive many artificial\n",
      "intelligence applications and services. Neural networks are composed of an input layer,\n",
      "one or more hidden layers, and an output layer, each layer in turn comprised of several\n",
      "nodes.\n",
      "Dropout regularizes neural networks by randomly dropping out nodes, along with their\n",
      "input and output connections, from the network during training (Fig. 3). Dropout trains\n",
      "several variations of a fixed-sized architecture, with each variation having different\n",
      "randomized nodes left out of the architecture. A single neural net without dropout is used\n",
      "for testing, employing an approximate averaging method derived from the randomly\n",
      "modified training architectures. In this way, dropout approximates training large a\n",
      "quantity of neural networks with a multitude of diversified architectures.9 \n",
      "\n",
      "Metadata: {'total_pages': 8, 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'creationdate': '2024-01-27T05:53:28+00:00', 'creationDate': \"D:20240127055328+00'00'\", 'format': 'PDF 1.4', 'moddate': '2024-01-27T05:53:28+00:00', 'title': '', 'page': 6, 'subject': '', 'trapped': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'keywords': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'producer': 'Skia/PDF m120', 'author': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(Document(id='a6eef5fa-18a5-464e-b553-f8c0efafb198', metadata={'source': 'kb_ip\\\\Foundations of LLMs.pdf', 'keywords': '', 'producer': '', 'page': 64, 'trapped': '', 'total_pages': 277, 'format': 'PDF 1.5', 'creator': '', 'author': '', 'title': '', 'file_path': 'kb_ip\\\\Foundations of LLMs.pdf', 'creationDate': '', 'moddate': '', 'creationdate': '', 'modDate': '', 'subject': ''}, page_content='58\\nGenerative Models\\nmodifications and improvements to the standard Transformer architecture, which are considered\\nimportant in developing trainable LLMs.\\n2.2.2.1\\nLayer Normalization with Residual Connections\\nLayer normalization is used to stabilize training for deep neural networks. It is a process of\\nsubtracting the mean and dividing by the standard deviation. By normalizing layer output in\\nthis way, we can effectively reduce the covariate shift problem and improve the training stability.\\nIn Transformers, layer normalization is typically used together with residual connections. As\\ndescribed in Section 2.1.1, a sub-layer can be based on either the post-norm architecture, in which\\nlayer normalization is performed right after a residual block, or the pre-norm architecture, in\\nwhich layer normalization is performed inside a residual block. While both of these architectures\\nare widely used in Transformer-based systems [Wang et al., 2019], the pre-norm architecture has\\nproven to be especially useful in training deep Transformers. Given this, most LLMs are based on\\nthe pre-norm architecture, expressed as output = LNorm(F(input)) + input.\\nA widely-used form of the layer normalization function is given by\\nLNorm(h)\\n=\\nŒ± ¬∑ h ‚àí¬µ\\nœÉ + œµ + Œ≤\\n(2.23)\\nwhere h is a d-dimensional real-valued vector, ¬µ is the mean of all the entries of h, and œÉ is the\\ncorresponding standard deviation. œµ is introduced for the sake of numerical stability. Œ± ‚ààRd and\\nŒ≤ ‚ààRd are the gain and bias terms.\\nA variant of layer normalization, called root mean square (RMS) layer normalization, only\\nre-scales the input vector but does not re-center it [Zhang and Sennrich, 2019]. The RMS layer\\nnormalization function is given by\\nLNorm(h)\\n=\\nŒ± ¬∑\\nh\\nœÉrms + œµ + Œ≤\\n(2.24)\\nwhere œÉrms is the root mean square of h, that is, œÉrms = ( 1\\nd\\nPd\\nk=1 h2\\nk)\\n1\\n2 . This layer normalization\\nfunction is used in LLMs like the LLaMA series.\\n2.2.2.2\\nActivation Functions in FFNs\\nIn Transformers, FFN sub-layers are designed to introduce non-linearities into representation\\nlearning, and are found to be useful for preventing the representations learned by self-attention\\nfrom degeneration8 [Dong et al., 2021]. A standard form of the FFNs used in these sub-layers can\\nbe expressed as\\nFFN(h)\\n=\\nœÉ(hWh + bh)Wf + bf\\n(2.25)\\nwhere Wh ‚ààRd√ódh, bh ‚ààRdh, Wf ‚ààRdh√ód, and bf ‚ààRd are the parameters, and dh is the\\nhidden size. œÉ(¬∑) is the activation function of the hidden layer. A common choice for œÉ(¬∑) is the\\n8Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.'), 0)\n",
      "Content: 58\n",
      "Generative Models\n",
      "modifications and improvements to the standard Transformer architecture, which are considered\n",
      "important in developing trainable LLMs.\n",
      "2.2.2.1\n",
      "Layer Normalization with Residual Connections\n",
      "Layer normalization is used to stabilize training for deep neural networks. It is a process of\n",
      "subtracting the mean and dividing by the standard deviation. By normalizing layer output in\n",
      "this way, we can effectively reduce the covariate shift problem and improve the training stability.\n",
      "In Transformers, layer normalization is typically used together with residual connections. As\n",
      "described in Section 2.1.1, a sub-layer can be based on either the post-norm architecture, in which\n",
      "layer normalization is performed right after a residual block, or the pre-norm architecture, in\n",
      "which layer normalization is performed inside a residual block. While both of these architectures\n",
      "are widely used in Transformer-based systems [Wang et al., 2019], the pre-norm architecture has\n",
      "proven to be especially useful in training deep Transformers. Given this, most LLMs are based on\n",
      "the pre-norm architecture, expressed as output = LNorm(F(input)) + input.\n",
      "A widely-used form of the layer normalization function is given by\n",
      "LNorm(h)\n",
      "=\n",
      "Œ± ¬∑ h ‚àí¬µ\n",
      "œÉ + œµ + Œ≤\n",
      "(2.23)\n",
      "where h is a d-dimensional real-valued vector, ¬µ is the mean of all the entries of h, and œÉ is the\n",
      "corresponding standard deviation. œµ is introduced for the sake of numerical stability. Œ± ‚ààRd and\n",
      "Œ≤ ‚ààRd are the gain and bias terms.\n",
      "A variant of layer normalization, called root mean square (RMS) layer normalization, only\n",
      "re-scales the input vector but does not re-center it [Zhang and Sennrich, 2019]. The RMS layer\n",
      "normalization function is given by\n",
      "LNorm(h)\n",
      "=\n",
      "Œ± ¬∑\n",
      "h\n",
      "œÉrms + œµ + Œ≤\n",
      "(2.24)\n",
      "where œÉrms is the root mean square of h, that is, œÉrms = ( 1\n",
      "d\n",
      "Pd\n",
      "k=1 h2\n",
      "k)\n",
      "1\n",
      "2 . This layer normalization\n",
      "function is used in LLMs like the LLaMA series.\n",
      "2.2.2.2\n",
      "Activation Functions in FFNs\n",
      "In Transformers, FFN sub-layers are designed to introduce non-linearities into representation\n",
      "learning, and are found to be useful for preventing the representations learned by self-attention\n",
      "from degeneration8 [Dong et al., 2021]. A standard form of the FFNs used in these sub-layers can\n",
      "be expressed as\n",
      "FFN(h)\n",
      "=\n",
      "œÉ(hWh + bh)Wf + bf\n",
      "(2.25)\n",
      "where Wh ‚ààRd√ódh, bh ‚ààRdh, Wf ‚ààRdh√ód, and bf ‚ààRd are the parameters, and dh is the\n",
      "hidden size. œÉ(¬∑) is the activation function of the hidden layer. A common choice for œÉ(¬∑) is the\n",
      "8Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing. \n",
      "\n",
      "Metadata: {'source': 'kb_ip\\\\Foundations of LLMs.pdf', 'keywords': '', 'producer': '', 'page': 64, 'trapped': '', 'total_pages': 277, 'format': 'PDF 1.5', 'creator': '', 'author': '', 'title': '', 'file_path': 'kb_ip\\\\Foundations of LLMs.pdf', 'creationDate': '', 'moddate': '', 'creationdate': '', 'modDate': '', 'subject': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search by Score: [Without Re-ranking]\n",
    "\n",
    "results = vectorstore.similarity_search_with_relevance_scores(query=\"What is regularization?\", k=k)\n",
    "\n",
    "for res in results:\n",
    "    print(res)\n",
    "    print(f\"Content: {res[0].page_content} \\n\\nMetadata: {res[0].metadata}\\n\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad69732-9528-4dbd-8e56-1c095b194c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d2e76f-7dd4-4592-9fbd-e781a97be584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 20 documents for re-ranking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content: Regularization. What, Why, When,\n",
      "and How?\n",
      "In this article, I want to take an in-depth look at regularization.\n",
      "Akash Shastri ¬∑ Follow\n",
      "Published in Towards Data Science ¬∑ 5 min read ¬∑ Oct 24, 2020\n",
      "132\n",
      "What?\n",
      "What is regularization?\n",
      "Regularization is a method to constraint the model to fit our data accurately\n",
      "and not overfit. It can also be thought of as penalizing unnecessary\n",
      "complexity in our model. There are mainly 3 types of regularization\n",
      "techniques deep learning practitioners use. They are:\n",
      "1. L1 Regularization or Lasso regularization\n",
      "2. L2 Regularization or Ridge regularization\n",
      "3. Dropout\n",
      "Sidebar: Other techniques can also have a regularizing effect on our model. You\n",
      "can prevent overfitting by also having more data to constraint the search space of\n",
      "our function. This can be done with techniques like data augmentation, that\n",
      "create more data to train, hence reducing overfitting.\n",
      "There are many other solutions to overfitting such as ensembling and stopping\n",
      "early, that can help prevent overfitting but are not considered regularization as \n",
      "\n",
      "Metadata: {'id': 2, 'relevance_score': np.float32(0.99982506), 'creationdate': '2024-01-27T06:07:53+00:00', 'moddate': '2024-01-27T06:07:53+00:00', 'subject': '', 'total_pages': 5, 'format': 'PDF 1.4', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'producer': 'Skia/PDF m120', 'keywords': '', 'trapped': '', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'creationDate': \"D:20240127060753+00'00'\", 'modDate': \"D:20240127060753+00'00'\", 'title': '', 'author': '', 'page': 0, 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Re-ranked Score: 0.9998250603675842\n",
      "\n",
      "\n",
      "Content: Published:¬†16 November 2023\n",
      "Contributors: Jacob Murel Ph.D., Eda Kavlakoglu\n",
      "Regularization is a set of methods for reducing overfitting in machine\n",
      "learning models. Typically, regularization trades a marginal decrease in\n",
      "training accuracy for an increase in generalizability.\n",
      "Regularization encompasses a range of techniques to correct for overfitting in machine\n",
      " As such, regularization is a method for increasing a model‚Äôs\n",
      "learning models.\n",
      "generalizability‚Äîthat is, it‚Äôs ability to produce accurate predictions on new datasets.1\n",
      "Regularization provides this increased generalizability at the sake of increased training\n",
      "error. In other words, regularization methods typically lead to less accurate predictions\n",
      "on training data but more accurate predictions on test data.\n",
      "Regularization differs from optimization. Essentially, the former increases model\n",
      "generalizability while the latter increases model training accuracy. Both are important\n",
      "concepts in machine learning and data science.\n",
      "There are many forms of regularization. Anything in the way of a complete guide requires\n",
      "a much longer book-length treatment. Nevertheless, this article provides an overview of\n",
      "the theory necessary to understand regularization‚Äôs purpose in machine learning as well\n",
      "as a survey of several popular regularization techniques.\n",
      "Bias-variance tradeoff\n",
      "Types of regularization with linear models\n",
      "Types of regularization in machine learning\n",
      "Products\n",
      "Resources\n",
      "Take the next step \n",
      "\n",
      "Metadata: {'id': 1, 'relevance_score': np.float32(0.9994469), 'creationdate': '2024-01-27T05:53:28+00:00', 'total_pages': 8, 'format': 'PDF 1.4', 'trapped': '', 'keywords': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'producer': 'Skia/PDF m120', 'title': '', 'subject': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'author': '', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'page': 1, 'moddate': '2024-01-27T05:53:28+00:00', 'creationDate': \"D:20240127055328+00'00'\"}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Re-ranked Score: 0.9994469285011292\n",
      "\n",
      "\n",
      "Content: Explore watsonx.ai\n",
      "What is regularization?\n",
      "Overview \n",
      "\n",
      "Metadata: {'id': 0, 'relevance_score': np.float32(0.9860281), 'title': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'page': 0, 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'author': '', 'format': 'PDF 1.4', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'modDate': \"D:20240127055328+00'00'\", 'total_pages': 8, 'subject': '', 'producer': 'Skia/PDF m120', 'creationdate': '2024-01-27T05:53:28+00:00', 'keywords': '', 'creationDate': \"D:20240127055328+00'00'\", 'moddate': '2024-01-27T05:53:28+00:00', 'trapped': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Re-ranked Score: 0.9860280752182007\n",
      "\n",
      "\n",
      "Content: When?\n",
      "When do we use regularization?\n",
      "We use regularization whenever we suspect our model is overfitting. The\n",
      "biggest signs of overfitting are the poor performance of validation metrics.\n",
      "The validation set is part of our dataset that the model has not yet seen.\n",
      "As we want to detect if our model is learning just from the data, or is being\n",
      "heavily influenced by noise, we use the validation set which has different\n",
      "noise than our training set. So if our model were to overfit the training data,\n",
      "it would predict poorly on our validation set.\n",
      "During training, we also constantly measure validation metrics. If we see the\n",
      "validation metrics not improving significantly, or worsening, this is a telltale\n",
      "sign that our model is overfitting. We need to then apply regularization\n",
      "techniques.\n",
      "Note: Some regularization techniques have no downside, and should be used ALL\n",
      "the time. An example of this is data augmentation. There‚Äôs no downside to using\n",
      "data augmentation and should be used regardless of whether model is overfitting.\n",
      "How?\n",
      "L1 Regularization\n",
      "L1 regularization works by adding a penalty based on the absolute value of\n",
      "parameters scaled by some value l (typically referred to as lambda).\n",
      "Initially our loss function was: Loss = f(preds,y)\n",
      "Where y is the target output, and preds is the prediction\n",
      "preds = WX + b, where W is parameters, X is input and b is bias.\n",
      "With L1 regularization we add an extra term of l*|W|, where W is the weight\n",
      "matrix (parameters). So our loss function after L1 regularization is \n",
      "\n",
      "Metadata: {'id': 3, 'relevance_score': np.float32(0.9536188), 'producer': 'Skia/PDF m120', 'moddate': '2024-01-27T06:07:53+00:00', 'keywords': '', 'total_pages': 5, 'trapped': '', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'title': '', 'creationdate': '2024-01-27T06:07:53+00:00', 'page': 2, 'author': '', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'subject': '', 'format': 'PDF 1.4', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'modDate': \"D:20240127060753+00'00'\", 'creationDate': \"D:20240127060753+00'00'\"}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Re-ranked Score: 0.9536188244819641\n",
      "\n",
      "\n",
      "Content: Regularization in Machine Learning\n",
      "Prashant Gupta ¬∑ Follow\n",
      "Published in Towards Data Science ¬∑ 7 min read ¬∑ Nov 15, 2017\n",
      "9.8K\n",
      "43\n",
      "One of the major aspects of training your machine learning model is\n",
      "avoiding overfitting. The model will have a low accuracy if it is overfitting. This\n",
      "happens because your model is trying too hard to capture the noise in your\n",
      "training dataset. By noise we mean the data points that don‚Äôt really represent\n",
      "the true properties of your data, but random chance. Learning such data\n",
      "points, makes your model more flexible, at the risk of overfitting. \n",
      "\n",
      "Metadata: {'id': 5, 'relevance_score': np.float32(0.9252148), 'subject': '', 'page': 0, 'total_pages': 6, 'source': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'modDate': \"D:20240120084858+00'00'\", 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'moddate': '2024-01-20T08:48:58+00:00', 'creationDate': \"D:20240120084858+00'00'\", 'author': '', 'format': 'PDF 1.4', 'producer': 'Skia/PDF m120', 'file_path': 'kb_ip\\\\Regularization in Machine Learning _ by Prashant Gupta _ Towards Data Science.pdf', 'creationdate': '2024-01-20T08:48:58+00:00', 'trapped': '', 'keywords': '', 'title': ''}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Re-ranked Score: 0.9252148270606995\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Search by FlaskRank Re-Ranking\n",
    "\n",
    "from flashrank import Ranker, RerankRequest\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import FlashrankRerank\n",
    "\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "print(f\"Retrieved {k} documents for re-ranking.\")\n",
    "\n",
    "# Instantiate the Flashrank re-ranker\n",
    "reranker = FlashrankRerank(model=\"ms-marco-MiniLM-L-12-v2\", top_n=n)\n",
    "\n",
    "# Wrap your base retriever with the re-ranker\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# The query\n",
    "query = \"What is regularization?\"\n",
    "\n",
    "# Use the compression_retriever instead of the vector store directly\n",
    "results = compression_retriever.invoke(query)\n",
    "\n",
    "# Now, iterate through the re-ranked results\n",
    "for res in results:\n",
    "    # Print the document content and metadata, including the re-ranked score\n",
    "    print(f\"Content: {res.page_content} \\n\\nMetadata: {res.metadata}\\n\\n\\n\\n\")\n",
    "\n",
    "    # You can access the re-ranked score from the metadata\n",
    "    reranked_score = res.metadata.get(\"relevance_score\")\n",
    "    if reranked_score:\n",
    "        print(f\"Re-ranked Score: {reranked_score}\\n\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb42ac-bb3a-4b04-a962-af9fdf146ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddeb167d-d7fa-4124-afa3-cf062547b516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 20 documents for re-ranking.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Re-ranked Documents ---\n",
      "Rank 1:\n",
      "Content: Published:¬†16 November 2023\n",
      "Contributors: Jacob Murel Ph.D., Eda Kavlakoglu\n",
      "Regularization is a set of methods for reducing overfitting in machine\n",
      "learning models. Typically, regularization trades a marginal decrease in\n",
      "training accuracy for an increase in generalizability.\n",
      "Regularization encompasses a range of techniques to correct for overfitting in machine\n",
      " As such, regularization is a method for increasing a model‚Äôs\n",
      "learning models.\n",
      "generalizability‚Äîthat is, it‚Äôs ability to produce accurate predictions on new datasets.1\n",
      "Regularization provides this increased generalizability at the sake of increased training\n",
      "error. In other words, regularization methods typically lead to less accurate predictions\n",
      "on training data but more accurate predictions on test data.\n",
      "Regularization differs from optimization. Essentially, the former increases model\n",
      "generalizability while the latter increases model training accuracy. Both are important\n",
      "concepts in machine learning and data science.\n",
      "There are many forms of regularization. Anything in the way of a complete guide requires\n",
      "a much longer book-length treatment. Nevertheless, this article provides an overview of\n",
      "the theory necessary to understand regularization‚Äôs purpose in machine learning as well\n",
      "as a survey of several popular regularization techniques.\n",
      "Bias-variance tradeoff\n",
      "Types of regularization with linear models\n",
      "Types of regularization in machine learning\n",
      "Products\n",
      "Resources\n",
      "Take the next step \n",
      "Metadata: {'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'keywords': '', 'author': '', 'moddate': '2024-01-27T05:53:28+00:00', 'title': '', 'total_pages': 8, 'creationdate': '2024-01-27T05:53:28+00:00', 'producer': 'Skia/PDF m120', 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'trapped': '', 'creationDate': \"D:20240127055328+00'00'\", 'modDate': \"D:20240127055328+00'00'\", 'subject': '', 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'format': 'PDF 1.4', 'page': 1}\n",
      "\n",
      "\n",
      "Rank 2:\n",
      "Content: Regularization. What, Why, When,\n",
      "and How?\n",
      "In this article, I want to take an in-depth look at regularization.\n",
      "Akash Shastri ¬∑ Follow\n",
      "Published in Towards Data Science ¬∑ 5 min read ¬∑ Oct 24, 2020\n",
      "132\n",
      "What?\n",
      "What is regularization?\n",
      "Regularization is a method to constraint the model to fit our data accurately\n",
      "and not overfit. It can also be thought of as penalizing unnecessary\n",
      "complexity in our model. There are mainly 3 types of regularization\n",
      "techniques deep learning practitioners use. They are:\n",
      "1. L1 Regularization or Lasso regularization\n",
      "2. L2 Regularization or Ridge regularization\n",
      "3. Dropout\n",
      "Sidebar: Other techniques can also have a regularizing effect on our model. You\n",
      "can prevent overfitting by also having more data to constraint the search space of\n",
      "our function. This can be done with techniques like data augmentation, that\n",
      "create more data to train, hence reducing overfitting.\n",
      "There are many other solutions to overfitting such as ensembling and stopping\n",
      "early, that can help prevent overfitting but are not considered regularization as \n",
      "Metadata: {'moddate': '2024-01-27T06:07:53+00:00', 'producer': 'Skia/PDF m120', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'trapped': '', 'modDate': \"D:20240127060753+00'00'\", 'title': '', 'format': 'PDF 1.4', 'author': '', 'keywords': '', 'total_pages': 5, 'page': 0, 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'creationdate': '2024-01-27T06:07:53+00:00', 'creationDate': \"D:20240127060753+00'00'\", 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'subject': ''}\n",
      "\n",
      "\n",
      "Rank 3:\n",
      "Content: Loss = f(preds, y) + l*abs(W)\n",
      "L2 Regularization\n",
      "L2 regularization is very similar to L1 regularization, except the penalty term\n",
      "is the square of the parameters scaled by some factor l (lambda)\n",
      "Loss = f(preds, y) + l*(W)¬≤\n",
      "Difference between L1 and L2 regularization\n",
      "The difference between L1 and L2 regularization is that the gradients of the\n",
      "loss function with respect to parameters for L1 regularization are\n",
      "INDEPENDENT of parameters, so some parameters can be set all the way to\n",
      "zero, hence completely ignored.\n",
      "But in L2 regularization, the gradients of the loss function are DEPENDENT\n",
      "linearly on the parameters, so the parameters can never be zero. They only\n",
      "asymptotically approach zero. This means that no parameter is entirely\n",
      "ignored, and every parameter always has at least a very minimal effect on\n",
      "predictions.\n",
      "This difference is key to choosing the type of regularization, if you know you\n",
      "have useless features, L1 might be a better choice. If you want to consider all\n",
      "features, the L2 might be a better choice.\n",
      "Nuance: There is one nuance in deep learning where you can use a best of both\n",
      "worlds approach, by using both L1 and L2 regularization. This is called Elastic net\n",
      "Regularization.\n",
      "Dropout\n",
      "Dropout is an amazing regularization technique that works only on neural\n",
      "networks (as far as I know). The amazing idea of dropout is to randomly zero \n",
      "Metadata: {'total_pages': 5, 'producer': 'Skia/PDF m120', 'creationdate': '2024-01-27T06:07:53+00:00', 'creationDate': \"D:20240127060753+00'00'\", 'keywords': '', 'modDate': \"D:20240127060753+00'00'\", 'title': '', 'format': 'PDF 1.4', 'subject': '', 'page': 3, 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'author': '', 'trapped': '', 'moddate': '2024-01-27T06:07:53+00:00', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'}\n",
      "\n",
      "\n",
      "Rank 4:\n",
      "Content: When?\n",
      "When do we use regularization?\n",
      "We use regularization whenever we suspect our model is overfitting. The\n",
      "biggest signs of overfitting are the poor performance of validation metrics.\n",
      "The validation set is part of our dataset that the model has not yet seen.\n",
      "As we want to detect if our model is learning just from the data, or is being\n",
      "heavily influenced by noise, we use the validation set which has different\n",
      "noise than our training set. So if our model were to overfit the training data,\n",
      "it would predict poorly on our validation set.\n",
      "During training, we also constantly measure validation metrics. If we see the\n",
      "validation metrics not improving significantly, or worsening, this is a telltale\n",
      "sign that our model is overfitting. We need to then apply regularization\n",
      "techniques.\n",
      "Note: Some regularization techniques have no downside, and should be used ALL\n",
      "the time. An example of this is data augmentation. There‚Äôs no downside to using\n",
      "data augmentation and should be used regardless of whether model is overfitting.\n",
      "How?\n",
      "L1 Regularization\n",
      "L1 regularization works by adding a penalty based on the absolute value of\n",
      "parameters scaled by some value l (typically referred to as lambda).\n",
      "Initially our loss function was: Loss = f(preds,y)\n",
      "Where y is the target output, and preds is the prediction\n",
      "preds = WX + b, where W is parameters, X is input and b is bias.\n",
      "With L1 regularization we add an extra term of l*|W|, where W is the weight\n",
      "matrix (parameters). So our loss function after L1 regularization is \n",
      "Metadata: {'title': '', 'page': 2, 'creationdate': '2024-01-27T06:07:53+00:00', 'file_path': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf', 'author': '', 'moddate': '2024-01-27T06:07:53+00:00', 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'modDate': \"D:20240127060753+00'00'\", 'trapped': '', 'producer': 'Skia/PDF m120', 'total_pages': 5, 'format': 'PDF 1.4', 'subject': '', 'creationDate': \"D:20240127060753+00'00'\", 'keywords': '', 'source': 'kb_ip\\\\Regularization. What, Why, When, and How_ _ by Akash Shastri _ Towards Data Science.pdf'}\n",
      "\n",
      "\n",
      "Rank 5:\n",
      "Content: - Lasso regression (or L1 regularization) is a regularization technique that penalizes high-\n",
      "value, correlated coefficients. It introduces a regularization term (also called, penalty\n",
      "term) into the model‚Äôs sum of squared errors (SSE) loss function. This penalty term is the\n",
      "absolute value of the sum of coefficients. Controlled in turn by the hyperparameter\n",
      "lambda (Œª), it reduces select feature weights to zero. Lasso regression thereby removes\n",
      "multicollinear features from the model altogether.\n",
      "- Ridge regression (or L2 regularization) is regularization technique that similarly\n",
      "penalizes high-value coefficients by introducing a penalty term in the SSE loss function. It\n",
      "differs from lasso regression however. First, the penalty term in ridge regression is the\n",
      "squared sum of coefficients rather than the absolute value of coefficients. Second, ridge\n",
      "regression does not enact feature selection. While lasso regression‚Äôs penalty term can\n",
      "remove features from the model by shrinking coefficient values to zero, ridge regression\n",
      "only shrinks feature weights towards zero but never to zero.\n",
      "- Elastic net regularization essentially combines both ridge and lasso regression but\n",
      "inserting both the L1 and L2 penalty terms into the SSE loss function. L2 and L1 derive\n",
      "their penalty term value, respectively, by squaring or taking the absolute value of the sum\n",
      "of the feature weights. Elastic net inserts both of these penalty values into the cost\n",
      "function (SSE) equation. In this way, elastic net addresses multicollinearity while also\n",
      "enabling feature selection.6\n",
      "In statistics, these methods are also dubbed ‚Äúcoefficient shrinkage,‚Äù as they shrink\n",
      "predictor coefficient values in the predictive model. In all three techniques, the strength\n",
      "of the penalty term is controlled by lambda, which can be calculated using various cross-\n",
      "validation techniques.\n",
      "Types of regularization in machine\n",
      "learning\n",
      "Dataset\n",
      "Data augmentation is a regularization technique that modifies model training data. It\n",
      "expands the size of the training set by creating artificial data samples derived from pre-\n",
      "existing training data. Adding more samples to the training set, particularly of instances\n",
      "rare in real world data, exposes a model to a greater quantity and diversity of data from\n",
      "which it learns. Machine learning research has recently explored data augmentation for\n",
      "classifiers, particularly as a means of resolving imbalanced datasets.7 Data augmentation\n",
      "differs from synthetic data however. The latter involves creating new, artificial data while \n",
      "Metadata: {'format': 'PDF 1.4', 'author': '', 'keywords': '', 'producer': 'Skia/PDF m120', 'creationdate': '2024-01-27T05:53:28+00:00', 'creationDate': \"D:20240127055328+00'00'\", 'source': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'page': 4, 'file_path': 'kb_ip\\\\What is regularization_ _ IBM.pdf', 'total_pages': 8, 'creator': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0', 'modDate': \"D:20240127055328+00'00'\", 'title': '', 'moddate': '2024-01-27T05:53:28+00:00', 'trapped': '', 'subject': ''}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search by GPT Re-ranking\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.callbacks import OpenAICallbackHandler\n",
    "callback_handler = OpenAICallbackHandler()\n",
    "\n",
    "# Use a larger k for the initial retrieval\n",
    "query = \"What is regularization?\"\n",
    "\n",
    "candidate_documents = vectorstore.as_retriever(search_kwargs={\"k\": k}).invoke(query)\n",
    "print(f\"Retrieved {len(candidate_documents)} documents for re-ranking.\")\n",
    "\n",
    "def gpt_re_ranker(llm, query, documents, top_n=n):\n",
    "    \"\"\"\n",
    "    Re-ranks documents using a GPT-based LLM.\n",
    "    \n",
    "    Args:\n",
    "        llm: The LLM instance (e.g., ChatOpenAI).\n",
    "        query: The user's query string.\n",
    "        documents: A list of candidate documents (langchain.schema.Document).\n",
    "        top_n: The number of top documents to return.\n",
    "\n",
    "    Returns:\n",
    "        A list of re-ranked documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Prepare documents for the prompt\n",
    "    docs_text = \"\\n\\nDocument:\".join(\n",
    "        [f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(documents)]\n",
    "    )\n",
    "\n",
    "    # Define the prompt for the LLM\n",
    "    rerank_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are an expert at evaluating the relevance of document for the query. \"\n",
    "                \"You have to rank documents based on relevance to a query i.e. how well and how completely the document answers the query. \"\n",
    "                \"You will be given a list of documents and a query. \"\n",
    "                \"Beginning of a new document is denoted by 'Document:' \"\n",
    "                \"Rank the documents from most relevant to least relevant based on the query. \"\n",
    "                \"Provide the output as a JSON array of objects, where each object has the document number and a relevance score (1-10). \"\n",
    "                \"Example: [{{'doc_id': 1, 'score': 9}}, {{'doc_id': 3, 'score': 7}}]\"\n",
    "            ),\n",
    "            (\"human\", \"Query: {query}\\n\\nDocuments to rank:\\n{docs_text}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create the chain for the re-ranker\n",
    "    rerank_chain = rerank_prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Invoke the chain to get the re-ranked results\n",
    "    ranked_results = rerank_chain.invoke({\"query\": query, \"docs_text\": docs_text})\n",
    "    \n",
    "    # Sort the documents based on the re-ranked scores\n",
    "    # Create a dictionary to map doc_id to its score\n",
    "    doc_scores = {res['doc_id'] - 1: res['score'] for res in ranked_results}\n",
    "    \n",
    "    # Sort the original documents based on the new scores\n",
    "    sorted_docs = sorted(documents, key=lambda doc: doc_scores.get(documents.index(doc), 0), reverse=True)\n",
    "    \n",
    "    # Return the top N documents\n",
    "    return sorted_docs[:top_n]\n",
    "\n",
    "# Instantiate your LLM\n",
    "llm = ChatOpenAI(temperature=0,\n",
    "                model=\"gpt-5-nano\",\n",
    "                max_tokens=None,\n",
    "                timeout=None,\n",
    "                max_retries=2,\n",
    "                service_tier=\"flex\",\n",
    "                callbacks=[callback_handler])\n",
    "\n",
    "# Use the custom re-ranker\n",
    "reranked_docs = gpt_re_ranker(llm, query, candidate_documents, top_n=n)\n",
    "\n",
    "### Step 3: Display the re-ranked documents\n",
    "print(\"\\n--- Re-ranked Documents ---\")\n",
    "for i, doc in enumerate(reranked_docs):\n",
    "    score = doc.metadata.get(\"relevance_score\", \"N/A\")\n",
    "    print(f\"Rank {i+1}:\")\n",
    "    print(f\"Content: {doc.page_content} \\nMetadata: {doc.metadata}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "312016d1-073b-4e24-9f05-31a2f2288bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens Used: 9489\n",
      "Prompt Tokens: 6510\n",
      "Completion Tokens: 2979\n",
      "Total Cost (USD): $0.000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "# Retrieve token usage and cost\n",
    "print(f\"Total Tokens Used: {callback_handler.total_tokens}\")\n",
    "print(f\"Prompt Tokens: {callback_handler.prompt_tokens}\")\n",
    "print(f\"Completion Tokens: {callback_handler.completion_tokens}\")\n",
    "print(f\"Total Cost (USD): ${callback_handler.total_cost:.30f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bc97b-2f10-46ae-9049-b28f5b341e28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
